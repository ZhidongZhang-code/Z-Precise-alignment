{
    "./alignment_normalization.py": "import sys\nimport re\nimport argparse\nimport logging\nimport yaml \nfrom log_config import setup_logging\n\n###序列比对结果输出文件标准化，输入的序列比对结果：-outfmt', '6 qseqid sseqid qlen slen qstart qend sstart send evalue bitscore length pident\n###筛选cov大于70%，id大于30的结果\n\n##输入文件格式：\n#0                          1                                             2        3        4       5       6      7            8           9        10     11      \n#qseqid                     sseqid                                       qlen    slen    qstart  qend    sstart  send       evalue      bitscore length pident\n#K00097_**_eco:b0052     GCF_000190995.1_ASM19099v1_**_WP_000241242.1    329     329     1       329     1       329     1.9e-182        644.8   329     325\n\n# 加载YAML配置文件的函数\ndef load_config(config_file):\n    with open(config_file, 'r') as file:\n        return yaml.safe_load(file)\n    \n    \ndef filenormalization(input_file, output_file, coverage_threshold, identity_threshold, logger):\n#    coverage_threshold = config['coverage_threshold']\n#    identity_threshold = config['identity_threshold']\n\n    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n        new_lines = []\n        for line in infile:\n            line = line.strip()\n            if re.match(r'^\\w+', line):\n                test = line.split('\\t')\n                if (abs(float(test[5]) - float(test[4]) + 1) / int(test[2]) >= coverage_threshold and\n                    abs(float(test[7]) - float(test[6]) + 1) / int(test[3]) >= coverage_threshold and\n                    float(test[11]) >= identity_threshold):\n                    new_lines.append(line + '\\n')\n\n        for line in new_lines:\n            parts = line.strip().split('\\t')\n            if len(parts) >= 2:\n                col1, col2 = parts[0], parts[1]\n                col1 = col1.split(\"_**_\")[0]\n                col2 = \"_\".join(col2.split(\"_\", 2)[:2])\n                new_line = \"{}\\t{}\\t{}\\n\".format(col2, col1, '\\t'.join(parts[2:]))\n                outfile.write(new_line)\n        logger.info(\"File normalization processed successfully.\")\n\n    logger.info(f\"Processing complete. Results have been written to {output_file}\")\n\ndef main():\n    parser = argparse.ArgumentParser(description='File normalization script for alignment result files.')\n    parser.add_argument('--alignmentout', type=str, required=True, help='Input file name , alignment result')\n    parser.add_argument('--normailzationout', type=str, required=True, help='normailzation output file name')\n    parser.add_argument('--coverage_threshold', type=float, help='Coverage threshold (default 0.7)')\n    parser.add_argument('--identity_threshold', type=float, help='Identity threshold (default 30)')\n    parser.add_argument('--log', type=str, help='Log file path (optional)')\n    parser.add_argument('--config', type=str, default='config.yaml', help='Configuration file path (default: config.yaml)')\n    args = parser.parse_args()\n\n    # 加载配置\n    config = load_config(args.config)\n\n    # 配置日志并获取 logger 实例\n    logger = setup_logging(args.log)\n\n    # 使用配置文件中的默认值，如果命令行中提供了值，则覆盖\n    coverage_threshold = args.coverage_threshold if args.coverage_threshold is not None else config['coverage_threshold']\n    identity_threshold = args.identity_threshold if args.identity_threshold is not None else config['identity_threshold']\n\n    # 调用文件处理函数\n    filenormalization(args.alignmentout, args.normailzationout, coverage_threshold, identity_threshold, logger)\n\n\nif __name__ == \"__main__\":\n    main()",
    "./blast_module.py": "import subprocess\nimport argparse\nimport logging\nimport sys\nimport yaml\nfrom log_config import setup_logging\n\n# 加载YAML配置文件的函数\ndef load_config(config_file):\n    with open(config_file, 'r') as file:\n        return yaml.safe_load(file)\n\ndef run_blast(query_file, output_file, blast_params, logger):\n    logger.info('BLAST analysis started.')\n\n    # 根据 blast_type 构建命令行参数\n    command = [\n        blast_params['blast_path'],  # 根据配置文件和用户选择确定BLAST程序的路径\n        '-db', blast_params['db_path'],\n        '-query', query_file,\n        '-out', output_file,\n        '-outfmt', '6 qseqid sseqid qlen slen qstart qend sstart send evalue bitscore length pident',\n        '-evalue', blast_params['evalue'],\n        '-max_target_seqs', blast_params['max_target_seqs'],\n        '-num_threads', str(blast_params['num_threads'])\n    ]\n\n    # 如果是 blast，添加矩阵参数\n    if blast_params['blast_type'] == 'blastp':\n        command.extend(['-matrix', blast_params['matrix']])\n\n    try:\n        subprocess.run(command, check=True)\n        logger.info(f\"{blast_params['blast_type']} command executed successfully.\")\n    except subprocess.CalledProcessError as e:\n        logger.error(f\"Error executing {blast_params['blast_type']} command: {e}\")\n        sys.exit(1)\n        \n    logger.info(f'{blast_params[\"blast_type\"]} analysis completed.')\n\ndef run():\n    parser = argparse.ArgumentParser(description='Run BLAST analysis.')\n    #parser.add_argument('--diamond_type',  default='diamond', help='Type of diamond BLAST program to use')\n    parser.add_argument('--blast_type', choices=['blastp', 'blastx'], default='blastp',required=True, help='Type of BLAST program to use')\n    parser.add_argument('--query', '-q', required=True, help='Path to the query file')\n    parser.add_argument('--outblastput', '-o', required=True, help='Path to the output file')\n    parser.add_argument('--evalue', '-e', default='1e-5', help='E-value threshold')\n    parser.add_argument('--max_target_seqs', default='5000', help='Maximum number of target sequences')\n    parser.add_argument('--matrix', default='BLOSUM62', help='Scoring matrix name (only used for blastp)')\n    parser.add_argument('--num_threads', '-t', default=8, help='Number of threads')\n    parser.add_argument('--log_file', '-l', help='Log file path (optional)')\n    parser.add_argument('--config', default='config.yaml', help='Configuration file path')\n    args = parser.parse_args()\n\n    # 加载配置\n    config = load_config(args.config)\n\n    # 使用 setup_logging 配置日志\n    logger = setup_logging(log_file_path=args.log_file)\n\n    # 将命令行参数整理成字典\n    blast_params = {\n        'blast_path': config[args.blast_type + '_path'],  # 根据选择的BLAST类型确定路径\n        'db_path': config['db_path'],\n        'evalue': args.evalue,\n        'max_target_seqs': args.max_target_seqs,\n        'matrix': args.matrix,\n        'num_threads': args.num_threads,\n        'blast_type': args.blast_type,\n    }\n\n    run_blast(args.query, args.outblastput, blast_params, logger)\n\nif __name__ == \"__main__\":\n    run()\n",
    "./config.yaml": "###############序列比对使用的工具#############\n#blast必须使用，选择是否使用HMM进行进一步的筛选(默认使用)\nhmmsearch: \"yes\" # [\"yes\" || \"NO\"]\npfam: \"yes\" # [\"yes\" || \"NO\"]\n\n################ bin ########################\n#blastp\nblastp_path: \"/public/home/TonyWuLab/zhangzhd/anaconda3/envs/blast/bin/blastp\"\n#blastx\nblastx_path: \"/public/home/TonyWuLab/zhangzhd/anaconda3/envs/blast/bin/blastx\"\n#clustalo\nclustalo_path: \"/public/home/TonyWuLab/zhangzhd/anaconda3/envs/clustalo/bin/clustalo\"\n#hmmsearch\nhmmsearch_path: \"/public/home/TonyWuLab/zhangzhd/anaconda3/bin/hmmsearch\"\n#hmmscan\nhmmscan_path: \"/public/home/TonyWuLab/zhangzhd/anaconda3/bin/hmmscan\"\n#hmmbuild\nhmmbuild: \"/public/home/TonyWuLab/zhangzhd/anaconda3/bin/hmmbuild\"\n#pfam\npfam_scan: \"/public/home/TonyWuLab/zhangzhd/software/pfam_scan-main/pfam_scan.py\"\n\n################ 数据库位置 ##################\n# 蛋白质blast目录\ndb_path: \"/public/group_share_data/TonyWuLab/zzd/db/medusa-2023/all-hum-medusa-db-genmoic-protein\"\n# GCF目录位置\nGCF_directory: \"/public/group_share_data/TonyWuLab/zzd/db/medusa-taxon-genmoic/hum-medusa-db-all\"\n# pfam目录位置\npfam_dirextory: \"/public/group_share_data/TonyWuLab/zzd/db/pfamdb/\"\n\n################ 序列比对参数阈值设置 #########(默认是这个，可以在脚本中指定)\n# blast\ncoverage_threshold: 0.7\nidentity_threshold: 30\n# hmmsearch\nhmm_evalue : 0.0001\nhmm_score : 200\n\n################ GCF的物种注释信息 ############\nGCF_annotation: \"/public/group_share_data/TonyWuLab/zzd/db/medusa-2023/all-hum-medusa-db-GCF-name.txt\"\nmedusa-annotation: \"/public/group_share_data/TonyWuLab/zzd/db/medusa-2023/medus-annotation.Latest.txt\"",
    "./excat_site_cds.py": "\"\"\"\n根据输入文件，提取目标位点附近上游24个，下游25个氨基酸位置的核苷酸序列\n用来构建bowtie2的索引\n\n###输入文件格式\nWP_003018763.1,GCF_029104325.1_ASM2910432v1,383\nWP_006234258.1,GCF_000169035.1_ASM16903v1,369\nWP_013451841.1,GCF_000183405.1_ASM18340v1,359\nWP_013479277.1,GCF_000175215.2_ASM17521v2,374\nWP_006042661.1,GCF_000153285.1_ASM15328v1,373\n\n\n\"\"\"\n\ndef parse_cds_fasta(filename):\n    \"\"\"\n    解析output_cds.fasta文件\n    返回一个字典，其中键是CDS的ID，值是序列\n    \"\"\"\n    cds_entries = {}\n    with open(filename, 'r') as file:\n        current_id = None\n        current_sequence = ''\n        for line in file:\n            if line.startswith('>'):\n                if current_id is not None:\n                    cds_entries[current_id] = current_sequence\n                current_id = line.strip().split(' ')[0][1:]\n                current_sequence = ''\n            else:\n                current_sequence += line.strip()\n        if current_id is not None:\n            cds_entries[current_id] = current_sequence\n    return cds_entries\n\ndef extract_nucleotide_sequence(cds_entries, protein_id, genome_id, position):\n    \"\"\"\n    从解析的CDS条目中提取特定蛋白质ID和基因组ID对应的核苷酸序列。\n\n    参数:\n    cds_entries (dict): 解析的CDS条目，键是CDS的ID，值是序列。\n    protein_id (str): 需要提取序列的蛋白质ID。\n    genome_id (str): 需要提取序列的基因组ID。\n    position (int): 蛋白质序列中感兴趣的氨基酸的位置。\n\n    返回:\n    str 或 None: 如果找到匹配的序列，则返回核苷酸序列字符串；否则返回None。\n    \"\"\"\n    for cds_id, sequence in cds_entries.items():\n        if protein_id in cds_id and genome_id in cds_id:\n            cds_position = int(position) * 3  # 一个氨基酸由三个核苷酸组成\n            start = max(0, cds_position - 75)\n            end = min(len(sequence), cds_position + 75)\n            nucleotide_sequence = sequence[start:end]\n            return cds_position,nucleotide_sequence\n    return None\n\ndef process_cds_fasta(cds_filename, matches_filename, output_filename):\n    \"\"\"\n    处理CDS FASTA文件和匹配结果文件，将匹配的核苷酸序列保存到指定的输出文件。\n\n    参数:\n    cds_filename (str): CDS FASTA文件的路径。\n    matches_filename (str): 匹配结果文件的路径。\n    output_filename (str): 输出文件的路径，用于保存匹配的核苷酸序列信息。\n\n    返回:\n    None: 将匹配的核苷酸序列信息保存到指定的输出文件。\n    \"\"\"\n    \n    cds_entries = parse_cds_fasta(cds_filename)\n    with open(matches_filename, 'r') as matches_file, open(output_filename, 'w') as output_file:\n        for line in matches_file:\n            protein_id, genome_id, position = line.strip().split(',')\n            result = extract_nucleotide_sequence(cds_entries, protein_id, genome_id, position)\n            if result is not None:\n                cds_position, nucleotide_sequence = result\n                position_int = int(position)  # 将字符串转换为整数\n                output_file.write(f\"{genome_id}_**_{protein_id}||{position_int - 24}:{position_int + 25}\\n{nucleotide_sequence}\\n\")\n            else:\n                # 如果你想在没有找到匹配项时做一些操作，可以在这里添加代码\n                pass",
    "./excat_site_protein.py": "\"\"\"\n根据输入样本格式，提取目标位点在GCF中的蛋白上**完整的**蛋白序列\n这是由于蛋白不用与构建bowtie2的数据库，一般保留完整的就可以\n\n输入样本格式：\n>WP_008514196.1 RYSRTQYFMD\n>WP_013696551.1 RYTRTRDCFE\n>WP_011095314.1 RYTRTRDLYD\n>WP_013636509.1 RYSRLSSLFD\n\n\n使用方法：\n1. 确保输入文件和FASTA文件按照上述格式准备好。\n2. 运行脚本，指定输入文件和FASTA文件的路径。\n3. 查看输出文件，获取匹配序列的详细信息。\n\n注意：此脚本需要Python环境支持，且依赖于标准的文件读写操作。\n\"\"\"\n\nimport argparse\nfrom log_config import setup_logging\n\ndef read_names_and_queries(filename):\n    \"\"\"\n    从文件中读取名称和查询序列。\n    \"\"\"\n    names = []\n    queries = {}\n    with open(filename, 'r') as file:\n        for line in file:\n            if line.startswith('>'):\n                name, query = line.strip().split()\n                name = name[1:]  # 移除\">\"\n                names.append(name)\n                queries[name] = query\n    return names, queries\n\ndef find_first_aa_position(target_sequence, query_sequence):\n    \"\"\"\n    在目标序列中查找查询序列的第一个氨基酸位置。\n    \"\"\"\n    return target_sequence.find(query_sequence) + 1  # 返回位置，从1开始计数\n\ndef process_site_sequences(input_filename, fasta_filename, output_filename):\n    \"\"\"\n    处理序列，找出匹配的序列及其位置，并将结果写入文件。\n    \"\"\"\n    # 读取名称和查询序列\n    names, queries = read_names_and_queries(input_filename)\n\n    # 打开FASTA文件，并查找所有匹配的序列\n    matches = []\n    with open(fasta_filename, 'r') as fasta_file:\n        record = None\n        for line in fasta_file:\n            if line.startswith('>'):\n                # 如果这一行是一个新的记录的开始，检查之前的记录是否匹配\n                if record:\n                    for name in names:\n                        if name in record[0]:\n                            # 在匹配的序列中查找查询序列的位置\n                            position = find_first_aa_position(''.join(record[1]), queries[name])\n                            matches.append((name, record[0], position))\n                            break\n                # 开始新的记录\n                record = (line, [])\n            else:\n                # 如果这是序列的一部分，添加到当前记录\n                record[1].append(line.strip())\n\n        # 检查最后一个记录是否匹配\n        if record:\n            for name in names:\n                if name in record[0]:\n                    position = find_first_aa_position(''.join(record[1]), queries[name])\n                    matches.append((name, record[0], position))\n                    break\n\n    # 将结果保存到文件\n    with open(output_filename, 'w') as output_file:\n        for match in matches:\n            output_file.write(f\"{match[0]},{match[1].split('_**_')[0][1:].strip()},{match[2]}\\n\")\n\n# 防止脚本直接运行\nif __name__ == \"__main__\":\n    print(\"This script is not intended to be run directly.\")\n",
    "./fasta_renamer.py": "import os\nimport sys\nimport argparse\nimport logging\nfrom log_config import setup_logging\n\n###将属于同一类型的蛋白质放在同一个文件中，并且加上_**_\n###结尾必须为fasta\n###使用方式python fatsa_renamer  --input Mura1.fasta Mura2.fasta --output Mura.fasta\n\n\ndef rename(input_files, output_file_path):\n    \"\"\"\n    批量处理输入的 FASTA 文件，为每个文件中的序列头部添加文件名作为前缀。\n\n    :param input_files: 输入文件的路径列表，期望为 FASTA 格式。\n    :param output_file_path: 处理后的序列将被追加到此输出文件中。\n    \"\"\"\n    for input_file_path in input_files:\n        if not input_file_path.endswith(\".fasta\"):\n            logging.warning(f\"Ignoring non-fasta file: {input_file_path}\")\n            continue\n\n        if not os.path.isfile(input_file_path):\n            logging.error(f\"File not found: {input_file_path}\")\n            continue\n\n        file_name = os.path.splitext(os.path.basename(input_file_path))[0]\n        modified_lines = []\n\n        with open(input_file_path, 'r') as input_file:\n            for line in input_file:\n                line = line.strip()\n                if line.startswith('>') and '_**_' not in line:\n                    modified_lines.append(f'>{file_name}_**_{line[1:]}')\n                else:\n                    modified_lines.append(line)\n\n        modified_content = '\\n'.join(modified_lines)\n\n        with open(output_file_path, 'a') as output_file:\n            output_file.write(modified_content)\n\n        logging.info(f\"Processed: {input_file_path}\")\n\n    logging.info(f\"All files have been processed. Output saved to: {output_file_path}\")\n\ndef main():\n    parser = argparse.ArgumentParser(description='Process input FASTA files and modify headers.')\n    parser.add_argument('--fasta_input', nargs='+', type=str, help='Input FASTA file paths')\n    parser.add_argument('--fasta_rename', type=str, help='Output FASTA file path')\n    parser.add_argument('--log', type=str, help='Log file path (optional)')\n    args = parser.parse_args()\n\n    output_file_path = args.fasta_rename\n    input_files = args.fasta_input\n\n    # 使用 setup_logging 配置日志\n    logger = setup_logging(log_file_path=args.log_file)\n\n    rename(input_files, output_file_path)\n    logger.info('fasta_renamer analysis completed.')\n\nif __name__ == \"__main__\":\n    main()\n",
    "./FileOperation.py": "import os\n\ndef remove_file(file):\n    \"\"\"\n    删除指定文件。\n\n    Args:\n        file (str): 要删除的文件路径。\n    \"\"\"\n    if os.path.exists(file):\n        os.remove(file)\n\n# 加载YAML配置文件的函数\ndef load_config(config_file):\n    with open(config_file, 'r') as file:\n        return yaml.safe_load(file)",
    "./find_extract_cds_protein.py": "import os\nimport re\nimport argparse\nimport yaml \nfrom concurrent.futures import ThreadPoolExecutor\nfrom threading import Lock\nfrom log_config import setup_logging\n\n# 加载YAML配置文件的函数\ndef load_config(config_file):\n    with open(config_file, 'r') as file:\n        return yaml.safe_load(file)\n\n# 初始化锁\nwrite_lock_cds = Lock()\nwrite_lock_protein = Lock()\n\ndef delete_output_files(alignment_cds , alignment_protein):\n    \"\"\"初始化输出文件\"\"\"\n    if os.path.exists(alignment_cds):\n        os.remove(alignment_cds)\n    if os.path.exists(alignment_protein):\n        os.remove(alignment_protein)\n\"\"\" \ndef findextractcdsprotein(input_file, directory, num_threads ,\n                          alignment_cds = 'alignment_extract_cds.fasta', \n                          alignment_protein = 'alignment_extract_protein.fasta', \n                          config_file='config.yaml'):\n    \"\"\" \ndef findextractcdsprotein(input_file, num_threads, alignment_cds, alignment_protein, logger, config):\n    \"\"\"\n    处理输入文件，提取符合条件的标识符，并使用线程池并行处理序列文件。\n\n    参数：\n        input_file：包含数据的输入文件路径\n        num_threads：要使用的线程数\n    \"\"\"\n\n    logger.info('find extract cds and protein analysis started.')\n    identifiers = set()  # 用于存储唯一的标识符\n\n    directory = config['GCF_directory']  # 从配置文件中读取路径\n\n    with open(input_file, 'r') as infile:\n        for line in infile:\n            line = line.strip()\n            if re.match(r'^\\w+', line):\n                columns = line.split('\\t')\n                # 根据特定条件筛选标识符\n                if (abs(float(columns[5]) - float(columns[4]) + 1) / int(columns[2]) >= 0.7 and      #（qstar-qend）/qlen\n                        abs(float(columns[7]) - float(columns[6]) + 1) / int(float(columns[3])) >= 0.7 and      #（sstar-send）sqlen\n                        float(columns[11]) >= 30):      #identity\n                    identifier = columns[1]\n                    identifiers.add(identifier)\n    \n    logger.info(f'find extract cds and protein analysis completed.')\n    logger.info(f'complete coding sequences(cds) sequence is saved in {alignment_cds}')\n    logger.info(f'complete protein sequence is saved in {alignment_protein}')\n\n    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n        futures = []\n        for identifier in identifiers:\n            gcf_identifier = re.search(r'(GCF_\\d+\\.\\d+)', identifier).group(1)\n#            wp_identifier = re.search(r'(WP_\\d+\\.\\d+)', identifier).group(1)\n            folder_path = os.path.join(directory, gcf_identifier)\n            if os.path.exists(folder_path):\n                cds_file_path = os.path.join(folder_path, \"cds_from_genomic.fna\")\n                protein_file_path = os.path.join(folder_path, \"protein.faa\")\n\n                if os.path.exists(cds_file_path) and os.path.exists(protein_file_path):\n                    futures.append(executor.submit(process_sequence, identifier, cds_file_path, alignment_cds, write_lock_cds))\n                    futures.append(executor.submit(process_sequence, identifier, protein_file_path, alignment_protein, write_lock_protein))\n\n        # Optional: Wait for all futures to complete if needed\n        # for future in futures:\n        #     future.result()\n\ndef process_sequence(identifier, file_path, output_file, lock):\n    \"\"\"\n    处理单个序列文件，提取序列并写入到输出文件中。\n\n    参数：\n        identifier：标识符 GCF_000027225.1_ASM2722v1_**_WP_012989815.1\n        file_path：序列文件路径\n        output_file：输出文件路径\n        lock：线程锁\n    \"\"\"\n    header, sequence = extract_sequence(identifier, file_path)\n    if sequence:\n        with lock:\n            with open(output_file, 'a') as outfile:\n                outfile.write(f\"{header}\\n{sequence}\\n\")\n\ndef extract_sequence(identifier, file_path):\n    \"\"\"\n    从序列文件中提取特定标识符对应的序列。\n\n    参数：\n        identifier：标识符\n        file_path：序列文件路径\n\n    返回：\n        header：包含标识符的序列标题\n        sequence：提取的序列内容\n    \"\"\"\n    header = \"\"\n    sequence = \"\"\n    gcf_identifier_search = re.search(r'(GCF_\\d+\\.\\d+)', identifier)\n    wp_identifier_search = re.search(r'_\\*\\*_(.+?)$', identifier)  # 除了GCF_000022005.1_ASM2200v1_**_WP_002518002.1还有GCF_000022005.1_ASM2200v1_**_YP_002518002.1，筛选条件要改成这种\n    \n    # 确保搜索到了GCF和WP标识符\n    if gcf_identifier_search and wp_identifier_search:\n        gcf_identifier = gcf_identifier_search.group(1)\n        wp_identifier = wp_identifier_search.group(1)  # 提取 _**_ 后面的内容\n        \n        try:\n            with open(file_path, 'r') as file:\n                is_sequence_started = False\n                for line in file:\n                    # 如果行中同时包含GCF和提取的标识符，则开始提取序列\n                    if gcf_identifier in line and wp_identifier in line:\n                        header = line.strip()\n                        is_sequence_started = True\n                    elif is_sequence_started and line.startswith(\">\"):\n                        break\n                    elif is_sequence_started:\n                        sequence += line.strip()\n        except IOError as e:\n            print(f\"Error file {file_path}: {e}\")\n    else:\n        print(f\"Identifier format mismatch in {identifier}\")\n    return header, sequence\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"python3 find-extract-cds-protein.py input_file , Output two files output_cds.fasta and output_protein.fasta \")\n    parser.add_argument(\"--input_file\", help=\"Input file containing data.\")\n    parser.add_argument('--extractcds', type=str, default='alignment_extract_cds.fasta', help='complete coding sequences(cds) sequence is saved (default: alignment_extract_cds.fasta)')\n    parser.add_argument('--extractprotein', type=str, default='alignment_extract_protein.fasta', help='complete protein sequence is saved (default: alignment_extract_protein.fasta)')\n    parser.add_argument(\"--num_threads\", type=int, default=8, help=\"Number of threads to use.\")\n    parser.add_argument('--log_file', '-l', type=str, help='Log file path (optional)')\n    parser.add_argument('--config', type=str, default='config.yaml', help='Configuration file path (default: config.yaml)')\n    args = parser.parse_args()\n\n    alignment_cds = args.extractcds   #'alignment_extract_cds.fasta'\n    alignment_protein = args.extractprotein #'alignment_extract_protein.fasta'\n\n    # 初始化原始文件\n    delete_output_files(alignment_cds , alignment_protein)      \n    # 加载配置\n    config = load_config(args.config)\n    # 配置日志并获取 logger 实例\n    logger = setup_logging(args.log)\n\n    findextractcdsprotein(args.input_file, args.num_threads, alignment_cds ,alignment_protein, logger, config)\n\nif __name__ == \"__main__\":\n    main()\n",
    "./hmm_module.py": "import subprocess\nimport argparse\nimport yaml\nfrom log_config import setup_logging\nimport logging\nfrom FileOperation import remove_file\n\n# 加载YAML配置文件的函数\ndef load_config(config_file):\n    with open(config_file, 'r') as file:\n        return yaml.safe_load(file)\n\ndef run_command(command,log):\n    try:\n        result = subprocess.run(command, check=True, capture_output=True, text=True)\n        log.info(f\"Command executed successfully: {' '.join(command)}\")\n        #if result.stdout:\n        #    log.info(result.stdout)\n        if result.stderr:\n            log.error(result.stderr)\n    except subprocess.CalledProcessError as e:\n        log.error(f\"Error executing command: {' '.join(command)}\")\n        log.error(e)\n\ndef run_HMMprcess(clustalo_input, cpu, hmmsearch_output,hmmsearch_params,tmp_inputfasta_clustao,tmp_inputfasta_clustao_hmmbuild,logger):\n    # print(f\"clustalo_input: {clustalo_input}\")\n    # print(f\"cpu: {cpu}\")\n    # print(f\"hmmsearch_output: {hmmsearch_output}\")\n    # print(f\"hmmsearch_params: {hmmsearch_params}\")\n    # print(f\"tmp_inputfasta_clustao: {tmp_inputfasta_clustao}\")\n    # print(f\"tmp_inputfasta_clustao_hmmbuild: {tmp_inputfasta_clustao_hmmbuild}\")\n\n    # Step 1: Run clustalo\n    remove_file(tmp_inputfasta_clustao)\n    clustalo_command = [ hmmsearch_params['clustalo'], \"-i\", clustalo_input, \"-o\", tmp_inputfasta_clustao]\n    run_command(clustalo_command,logger)\n\n    # Step 2: Run hmmbuild\n    hmmbuild_command = [hmmsearch_params['hmmbuild'], tmp_inputfasta_clustao_hmmbuild, tmp_inputfasta_clustao]\n    run_command(hmmbuild_command,logger)\n\n    # Step 3: Run hmmsearch\n    hmmsearch_command = [hmmsearch_params['hmmsearch'], \"--cpu\", cpu, \"-o\", hmmsearch_output, tmp_inputfasta_clustao_hmmbuild, hmmsearch_params['hmm_db']]\n    run_command(hmmsearch_command,logger)\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description='Run clustalo, hmmbuild, and hmmsearch in sequence.')\n    parser.add_argument('--fasta_input', type=str, help='Input FASTA file path', metavar='FASTA_FILE')\n    parser.add_argument('--cpu', type=str, help='Number of CPUs to use for hmmsearch.', required=True)\n    parser.add_argument('--hmmsearch_output', type=str, help='Output file name for hmmsearch results.', required=True)\n    parser.add_argument('--config', type=str, default='config.yaml', help='Configuration file path (default: config.yaml)', metavar='CONFIG_FILE')\n    parser.add_argument('--log_file', type=str, required=True, help='Log file path')\n\n    args = parser.parse_args()\n\n    # 临时文件\n    tmp_inputfasta_clustao = \"tmp_inputfasta_clustao.fasta\"\n    tmp_inputfasta_clustao_hmmbuild = \"tmp_inputfasta_clustao.hmm\"\n\n    # 加载配置\n    config = load_config(args.config)\n\n    # 使用 setup_logging 配置日志\n    logger = setup_logging(args.log_file)\n\n    hmmsearch_params = {\n        'hmmsearch' : config['hmmsearch_path'] ,\n        'hmmscan' : config['hmmscan_path'] ,\n        'hmmbuild': config['hmmbuild'] ,\n        'clustalo' : config['clustalo_path'] ,\n        'hmm_db' : config['db_path']\n    }\n    #print(hmmsearch_params)\n    #独立脚本模块文件\n    tmp_hmmsearch_output = args.hmmsearch_output\n\n    run_HMMprcess(args.input, args.cpu, tmp_hmmsearch_output,hmmsearch_params,tmp_inputfasta_clustao,tmp_inputfasta_clustao_hmmbuild,logger)\n\n    # 临时文件删除\n    #remove_file(tmp_inputfasta_clustao)\n    #remove_file(tmp_inputfasta_clustao_hmmbuild)",
    "./log_config.py": "import logging\n\ndef setup_logging(log_file_path=None, log_level=logging.INFO):\n    \"\"\"\n    配置日志系统。\n\n    :param log_file_path: 日志文件的路径。如果未指定，则日志输出到控制台。\n    :param log_level: 日志级别，默认为 logging.INFO。\n    :return: 配置好的 logger 对象。\n    \"\"\"\n    logger = logging.getLogger('MyScriptLogger')\n    logger.setLevel(log_level)\n\n    # 移除已存在的所有处理器，避免重复日志\n    if logger.handlers:\n        for handler in logger.handlers:\n            logger.removeHandler(handler)\n\n    # 根据是否指定了日志文件路径来决定日志的输出方式\n    if log_file_path:\n        file_handler = logging.FileHandler(log_file_path)\n    else:\n        file_handler = logging.StreamHandler()\n\n    file_handler.setLevel(log_level)\n    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter)\n    logger.addHandler(file_handler)\n    return logger\n",
    "./main_alignment.py": "import argparse\nimport csv\nimport shutil\nfrom blast_module import setup_logging, run_blast, load_config  \nfrom FileOperation import remove_file\nfrom fasta_renamer import rename \nfrom alignment_normalization import filenormalization\nfrom find_extract_cds_protein import findextractcdsprotein\nfrom makeformat import makeformat\nfrom run_clustalo_process import read_alignment_from_file,find_start_end_positions,find_end_position,trim_alignment,write_alignment_to_file,filter_duplicate_proteins,merge_fasta_files,run_clustalo,remove_file\nfrom hmm_module import run_HMMprcess\nfrom process_hmmresult import excat_hmm_postition, compare_files_and_keep_df1_only\nfrom pfam_module import run_pfam\nfrom process_pfamresult import pfam_compare_files_and_keep_df1_only\n\n\n###blastout算是一个默认参数，最后需要删除\n\n###日志：\n###验证结果是否正确：可以与urda的150nt比较\n###还是没有解决rename.py的问题，这个脚本是不是应该单独拿出来？\n###除了blastp还应该可以支持blastx 由于数据库是蛋白库，所以不能支持blastn    完成\n###blastp的参数应该作为一个参数能在脚本中进行选择（cov、id、e值、k）    完成\n###如果某一部分出了问题，需要报错在那一部分出的错误 完成\n###hmm模块还没有装载好，并且还是考虑这个是否必须使用    完成 \n###在config中选择是否使用HMM，默认为使用    完成 \n###pfam模块也要加载 (1648个序列8线程跑半小时，有些慢)\n###pfam模块不需要每一次都跑一边，因为pfam的输入序列是固定的蛋白数据库中的，所有可以直接先跑完，跑出来pfam的那个表格，然后用blaast的相似序列直接去查找就可以了\n###但是还是需要引入pfam的模块的，因为之后如果在加入新的数据库，这个过程还需要继续改变！！！！\n###pfam是不是要提供全部的type用来参考？\n###每一个模块运行后，要给出运行这个模块的命令行脚本\n###模块的异常处理\n###参数列表检查是不是存在必须要有的数据库（参考pfam脚本）\n###还是得做GUI，在乌班图这种有图形化界面的linux系统上使用\n###测试完成后临时文件的删除\n\n__version__ = '1.0'\n\ndef main():\n    parser = argparse.ArgumentParser(description='Control script to run BLAST analysis')\n    parser = argparse.ArgumentParser(description=f'NBrun.py v.{__version__}')\n\n    # 创建参数分组\n    input_group = parser.add_argument_group('Input options')\n    output_group = parser.add_argument_group('Output options')\n    config_group = parser.add_argument_group('Configuration')\n    blast_group = parser.add_argument_group('Blast(default)')\n    HMMsearch_group = parser.add_argument_group('HMMsearch(default)')\n    clustal_group = parser.add_argument_group('clustal')\n    pfam_group = parser.add_argument_group('pfam')\n\n    # 输入选项\n    input_group.add_argument('--fasta_input', type=str, required=True, help='Input FASTA file path', metavar='FASTA_FILE')\n    input_group.add_argument('--target_sequence_file', type=str, required=True, help=\"File containing the target sequence (150nt)\", metavar='TARGET_SEQ_FILE')\n\n    # 输出选项\n    #output_group.add_argument('--blastoutput', '-o', type=str, help='Path to the BLAST output file', metavar='BLAST_OUTPUT')\n    output_group.add_argument('--normailzationout', type=str, required=True, help='Normalization output file name', metavar='NORM_OUTPUT')\n    output_group.add_argument('--extractcds', type=str, default='alignment_extract_cds.fasta', help='File where the complete CDS sequences are saved (default: alignment_extract_cds.fasta)', metavar='CDS_FILE')\n    output_group.add_argument('--extractprotein', type=str, default='alignment_extract_protein.fasta', help='File where the complete protein sequences are saved (default: alignment_extract_protein.fasta)', metavar='PROTEIN_FILE')\n    output_group.add_argument(\"--clustalo_extract_out\", type=str ,required=True,help=\"clustalo extract output file.\")\n\n    # 配置选项\n    config_group.add_argument('--num_threads', '-t', type=int, default=8, help='Number of threads to use (default: 8)', metavar='THREADS')\n    config_group.add_argument('--cpu', type=str, default=8, help='Number of CPUs to use for hmmsearch.')\n    config_group.add_argument('--log_file', '-l', type=str, help='Optional log file path', metavar='LOG_FILE')\n    config_group.add_argument('--config', type=str, default='config.yaml', help='Configuration file path (default: config.yaml)', metavar='CONFIG_FILE')\n    config_group.add_argument('--version', '-v', action='version',version=f'{__version__}',help=\"Print version information and exit\")\n\n    # pfam参数设置\n    pfam_group.add_argument('--pfamkey', nargs='+',type=str, help='One or more pfam domain keys , Example: \"Mur_ligase_C Another_HMM\" ')\n    pfam_group.add_argument('--pfamevalue', type=float, help='E-value threshold')\n\n    # clustalo参数设置\n    clustal_group.add_argument(\"--clustalo_out\", type=str ,help=\"clustalo output file.\")\n\n    # blast默认参数设置\n    blast_group.add_argument('--blast_type', choices=['blastp', 'blastx'], default='blastp', help='Type of BLAST program to use   (default:blastp)')\n    blast_group.add_argument('--evalue', '-e', default='1e-5', help='E-value threshold  (default:1e-5)')\n    blast_group.add_argument('--coverage_threshold', '-cov',type=float, help='Coverage threshold (default 0.7)')\n    blast_group.add_argument('--identity_threshold', '-id',type=float, help='Identity threshold (default 30)')\n    blast_group.add_argument('--max_target_seqs', default='5000', help='Maximum number of target sequences  (default:5000)')\n    blast_group.add_argument('--matrix', default='BLOSUM62', help='Scoring matrix name  (default:BLOSUM62)')\n\n    # hmmsearch默认参数设置\n    HMMsearch_group.add_argument('--hmm_evalue', default='1e-4',type=str, help='hmmsearch evalue  (default:1e-4)', metavar='evalue')\n    HMMsearch_group.add_argument('--hmm_score', default='200',type=int, help='hmmsearch score  (default:200)', metavar='score')\n\n    args = parser.parse_args()\n\n    ### 加载配置文件、log\n    logger = setup_logging(args.log_file)\n    config = load_config(args.config)  \n\n    ### 特殊参数处理\n    #blast 使用配置文件中的默认值，如果命令行中提供了值，则覆盖\n    coverage_threshold = args.coverage_threshold if args.coverage_threshold is not None else config['coverage_threshold']\n    identity_threshold = args.identity_threshold if args.identity_threshold is not None else config['identity_threshold']\n    \n    #HMM 使用配置文件中的默认值，如果命令行中提供了值，则覆盖\n    hmm_evalue = args.hmm_evalue if args.hmm_evalue is not None else config['hmm_evalue']\n    hmm_score = args.hmm_score if args.hmm_score is not None else config['hmm_score']\n\n    ### 中间临时文件\n    tmp_blastout = \"tmp_blastout.csv\"\n    normailzation_out = 'tmp-normailzation-out.csv'\n    temp_filtered_file = \"temp_filtered.fasta\"\n    temp_filtered_topfam_file = \"temp_filtered_topfam.fasta\"\n    temp_extractcds = 'tmp_extractcds.fasta'\n    temp_extractprotein = 'tmp_extractprotein.fasta'\n    tmp_inputfasta_clustao = \"tmp_inputfasta_clustao.fasta\"\n    tmp_inputfasta_clustao_hmmbuild = \"tmp_inputfasta_clustao.hmm\"\n    tmp_hmmsearch_output = 'hmmsearch_output.txt'\n    tmp_hmmsearch_excat = 'tmp_hmmsearch_excat.csv'\n    tmp_with_hmm_normailzationout = 'tmp_with_hmm_normailzationout.csv'\n    tmp_pfam_output = \"tmp_pfam_output.csv\"\n    merged_file = \"tmp_merged.fasta\"\n    clustal_output_file = \"tmp_clu-out.fasta\"\n    #tmp_pfam_result = \"tmp_pfam_result.csv\"\n    tmp_pfam_result = \"tmp_pfam_output.csv\"\n    tmp_pfam_excat = 'tmp_pfam_excat.csv'\n\n    # 判断中间文件是否保留\n    if args.clustalo_out :\n        clustal_output_file = args.clustalo_out\n\n    # 读取目标序列文件，获取序列ID\n    with open(args.target_sequence_file, 'r') as f:\n        target_sequence_id = f.readline().strip()\n\n    ### 将命令行参数整理成字典\n    #blast\n    blast_params = {\n        'blast_path': config[args.blast_type + '_path'],  # 根据选择的BLAST类型确定路径\n        'db_path': config['db_path'],\n        'evalue': args.evalue,\n        'max_target_seqs': args.max_target_seqs,\n        'matrix': args.matrix,\n        'num_threads': args.num_threads,\n        'blast_type': args.blast_type,\n    }\n    #HMM\n    hmmsearch_params = {\n        'hmmsearch' : config['hmmsearch_path'] ,\n        'hmmscan' : config['hmmscan_path'] ,\n        'hmmbuild': config['hmmbuild'] ,\n        'clustalo' : config['clustalo_path'] ,\n        'hmm_db' : config['db_path']\n    }\n    #pfam\n    pfam_params = {\n        'pfam_path': config['pfam_scan'],  \n        'pfamdb_path': config['pfam_dirextory'],\n        'cpu': args.cpu,\n        'out': tmp_pfam_output ,\n    }\n\n    ### 整体流程\n    #rename(args.fasta_input, args.fasta_output, logger)\n    ##BLAST序列比对\n    #run_blast(args.fasta_rename, tmp_blastout, args.num_threads, logger, config)\n    run_blast(args.fasta_input, tmp_blastout, blast_params, logger)\n    ##HMM进一步筛选比对结果\n    if config[\"hmmsearch\"] == \"yes\" : \n        #添加一步hmmsearch进行进一步的筛选\n        run_HMMprcess(args.fasta_input, args.cpu, tmp_hmmsearch_output,hmmsearch_params, tmp_inputfasta_clustao, tmp_inputfasta_clustao_hmmbuild, logger)\n        #调用函数执行比较和保存结果\n        excat_hmm_postition(tmp_hmmsearch_output, tmp_hmmsearch_excat, hmm_evalue, hmm_score)\n        compare_files_and_keep_df1_only(tmp_blastout, tmp_hmmsearch_excat, tmp_with_hmm_normailzationout, logger)\n        #覆盖原始的normailzationout\n        shutil.move(tmp_with_hmm_normailzationout, tmp_blastout)\n    ###pfam进行进一步筛选\n    if config[\"pfam\"] == \"yes\" : \n        #将这个蛋白结果提取出来，用于pfam的搜索\n        #findextractcdsprotein(args.blastoutput, args.num_threads, temp_extractcds ,temp_extractprotein , logger, config)\n        #进行筛选，将序列去重\n        #filter_duplicate_proteins(temp_extractprotein, temp_filtered_topfam_file)  # 过滤重复的蛋白质ID并写入临时文件\n        #使用pfam进行进一步筛选\n        run_pfam(temp_filtered_topfam_file,pfam_params, logger)   #调用pfam.py，结果保存在tmp_pfam_result\n        pfam_compare_files_and_keep_df1_only(tmp_pfam_result, tmp_blastout, args.pfamkey, tmp_pfam_output,logger)\n        #shutil.move(tmp_pfam_excat, args.blastoutput)\n        shutil.move(tmp_pfam_output, tmp_blastout)\n    # filenormalization(args.blastoutfile, args.normailzationout, coverage_threshold, identity_threshold, logger)\n    # makeformat(normailzation_out, args.normailzationout, logger,config)\n    findextractcdsprotein(tmp_blastout, args.num_threads, args.extractcds ,args.extractprotein, logger, config)\n    ##clustalo多序列比对\n    remove_file(merged_file)  # 删除之前的 merged.fasta 文件\n    filter_duplicate_proteins(args.extractprotein, temp_filtered_file)  # 过滤重复的蛋白质ID并写入临时文件\n    merge_fasta_files(temp_filtered_file, args.target_sequence_file, merged_file)   #融合、制作多序列比对文件\n    run_clustalo(merged_file, clustal_output_file, args.num_threads, logger, config)\n    #remove_file(merged_file)  # 删除临时合并的文件\n    #remove_file(temp_filtered_file)  # 删除临时过滤的文件\n    ##提取达标序列\n    alignment = read_alignment_from_file(clustal_output_file)\n    start_position, end_position = find_start_end_positions(alignment, target_sequence_id)\n    end_position = find_end_position(alignment, target_sequence_id)\n    trimmed_alignment = trim_alignment(alignment, start_position, end_position, target_sequence_id)\n    write_alignment_to_file(trimmed_alignment, args.clustalo_extract_out)\n\n    ### 临时文件删除\n    # 判断是否有需要保存的文件\n    if args.clustalo_out is None:\n        remove_file(clustal_output_file)\n\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "./main_excat.py": "import argparse\nimport yaml\nfrom position_sequence_matcher import parse_position_arg, find_sequences_with_rf\nfrom sequence_pattern_extractor import read_names_from_file, find_matching_sequences, save_matches_to_file\nfrom excat_site_protein import process_site_sequences\nfrom excat_site_cds import process_cds_fasta\nfrom FileOperation import remove_file\nfrom species_checker_format import SpeciesChecker\nfrom log_config import setup_logging\n\n###日志\n###修改cds输出格式，并且要注意会有没有找到的 ：No match found for protein ID WP_040344983.1, genome ID GCF_000165795.1_ASM16579v1, position 364 \n###还没想好怎么处理，在excat_site_cds的模块中  （完成）\n###规范化输出文件，该删除的删除   （完成）\n###异常处理！不是所有人都看代码，如果出错要让大家知道错误发生在什么上面\n###\n\n\n###新增规则：25：R表示在25位置为R，25！R表示在25位置不为R\n\n###python src/main_excat.py --input clu-extract-out.fasta --position 25:R 28:R --extractprotein blast_normailzation_excate_protein.fasta --extractcds blast_normailzation_excate_cds.fasta --site_protein_output site-protein.fasta --site_cds_output site-cds.fasta --formatoutput 111 --config ./src/config.yaml\n\n# 加载YAML配置文件的函数\ndef load_config(config_file):\n    with open(config_file, 'r') as file:\n        return yaml.safe_load(file)\n    \ndef main():\n    parser = argparse.ArgumentParser(description=\"Process protein sequences\")\n    \n    # 创建参数分组\n    input_group = parser.add_argument_group('Input options')\n    output_group = parser.add_argument_group('Output options')\n    config_group = parser.add_argument_group('Configuration')\n\n    #输入参数\n    input_group.add_argument(\"--input\",'-i', required=True, help=\"Input ClustalO FASTA file.\", metavar=\"FASTA\")\n    input_group.add_argument(\"--position\",required=True, nargs='+', type=str, help=\"Positions and corresponding amino acids.\", metavar=\"POS:AA\")\n    input_group.add_argument('--extractprotein', required=True, type=str, help='Output file for complete protein sequences.', metavar='PROTEIN_FILE')\n    input_group.add_argument('--extractcds', required=True, type=str, help='Output file for complete CDS sequences.', metavar='CDS_FILE') \n    #输出参数\n    output_group.add_argument(\"--site_protein_output\", required=True,  type=str, help=\"Output file for matched protein sequences.\", metavar=\"PROTEIN_OUT\")\n    output_group.add_argument(\"--site_cds_output\", required=True,  type=str, help=\"Output file for matched CDS sequences.\", metavar=\"CDS_OUT\")\n    output_group.add_argument('--formatoutput', type=str, required=True, help='Path to output file')\n    # 配置选项\n    config_group.add_argument('--config', type=str, required=True, default='config.yaml', help='Configuration file path (default: config.yaml)', metavar='CONFIG_FILE')\n    config_group.add_argument('--log_file', '-l', help='Log file path (optional)')\n\n    args = parser.parse_args()\n\n    #临时文件\n    tmp_position_swquence = \"tmp_position_swquence.fasta\"\n    tmp_matchesq =  \"tmp_matches.fasta\"\n\n    # 加载配置\n    config = load_config(args.config)\n\n    #日志、cfg文件模块加载\n    logger = setup_logging(args.log_file)\n\n    # 定义文件路径\n    fasta_file = args.input\n    #input_names_file = args.input_file\n    protein_db_file = args.extractprotein\n    #output_file_path = args.output_file \n\n    #  config传入文件\n    format_path = config['medusa-annotation']\n\n    # 输入序列位置信息  根据输入位点（25：R） 有这个位置的序列的下10个氨基酸\n    # >WP_011802784.1 RYSRTRDCFE\n    positions = parse_position_arg(args.position)\n    # 获取所有满足条件的序列的ID以及相应的锚点序列信息\n    rf_sequences = find_sequences_with_rf(fasta_file, positions)\n    #将得到的序列位置信息保存到文件 >WP_011802784.1 RYSRTRDCFE\n    with open(tmp_position_swquence, 'w') as out_file:\n        for sequence_id, anchor_seq in rf_sequences:\n            out_file.write(f\">{sequence_id} {anchor_seq}\\n\")\n    #print(f\"结果已保存到{tmp_position_swquence}文件中\")\n    # 读取输入序列的id\n    names = read_names_from_file(tmp_position_swquence)\n    # 根据names信息在蛋白质的结果里面找到对应的序列，做成表格\n    matches = find_matching_sequences(protein_db_file, names)\n    # 将所有匹配的序列保存到文件中\n    save_matches_to_file(matches, args.site_protein_output)#\n    #根据位点蛋白序列提取核苷酸位点信息  WP_003018763.1,GCF_024638115.1_ASM2463811v1,383 \n    process_site_sequences(tmp_position_swquence , args.extractprotein , tmp_matchesq)\n    #根据位点核苷酸序列提取\n    process_cds_fasta(args.extractcds, tmp_matchesq, args.site_cds_output)\n    # 制作表格\n    checker = SpeciesChecker(args.extractprotein, args.site_protein_output, format_path, args.formatoutput, logger)\n    checker.generate_report()\n\n\n    #移除中间文件\n    #remove_file(tmp_matchesq)\n    #remove_file(tmp_position_swquence)\n    #制作表格\n    \n\n\n\nif __name__ == \"__main__\":\n\n    main()\n",
    "./makeformat.py": "import argparse\nimport pandas as pd\nimport logging\nimport yaml \nfrom log_config import setup_logging\n\n#1制作表格\n#0                   1         2        3        4       5       6      7            8           9        10     11\n#qseqid              sseqid    qlen    slen    qstart  qend    sstart  send       evalue      bitscore length pident\n#GCF_000190995.1     K00097    329     329     1       329     1       329     1.9e-182        644.8     329     25\n\n# 加载YAML配置文件的函数\ndef load_config(config_file):\n    with open(config_file, 'r') as file:\n        return yaml.safe_load(file)\n\ndef makeformat(inputfile, outputfile, logger,config):\n\n    logger.info('BLASTP makeformat started.')\n    # 初始化一个字典来存储唯一的行\n    unique_lines = {}\n\n    with open(inputfile, 'r', encoding='utf-8') as fr:\n        for line in fr:\n            sp = line.strip().split(\"\\t\")\n            # 确保e-value满足条件\n            if float(sp[8]) < 0.00001:\n                # 构造唯一标识符\n                unique_key = f\"{sp[1]}_**_{sp[0]}\"  # sseqid_**_qseqid\n                # 如果这个唯一标识符还没有被添加到字典中，则添加它\n                if unique_key not in unique_lines:\n                    unique_lines[unique_key] = line.strip()  # 或者其他需要保留的数据形式\n\n    # 接下来的部分处理DataFrame的创建\n    index_list = []\n    column_list = []\n    index_columns = {}\n    for key in unique_lines.keys():\n        sp = key.split('_**_')  # K00097_**_GCF_000190995.1\n        sseqid, qseqid = sp[0], sp[1]\n        index_list.append(sseqid)\n        column_list.append(qseqid)\n        if sseqid not in index_columns:\n            index_columns[sseqid] = []\n        index_columns[sseqid].append(qseqid)\n\n    df = pd.DataFrame('', index=list(set(index_list)), columns=list(set(column_list)))\n\n    for index, column in zip(index_list, column_list):\n        df.loc[index, column] = 'yes'\n\n    df2 = pd.DataFrame(df.values.T, index=df.columns, columns=df.index)\n    tmp = {}\n    GCF_annotation = config['GCF_annotation']\n    with open(GCF_annotation, 'r', encoding='utf-8') as fr:\n        for line in fr:\n            sp1 = line.strip().split('\\t')\n            if sp1[0] not in tmp:\n                tmp[sp1[0]] = sp1[1]\n    df2['baname'] = ''\n    for index, row in df2.iterrows():\n        df2.loc[index, \"baname\"] = tmp.get(index, '')\n    c = df2.pop(\"baname\")\n    df2.insert(0, 'baname', c)\n    df2[\"sum\"] = (df2 == \"yes\").sum(axis=1)\n    d = df2.pop('sum')\n    df2.insert(1, 'sum', d)\n\n    # 保存到CSV\n    df2.to_csv(outputfile, sep='\\t', index=True, header=True)\n    logger.info('BLASTP makeformat end.')\n    logging.info(f\"File format saved to {outputfile}\")\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Process and transform biological data into a tabular format.\")\n    parser.add_argument(\"--inputfile\", type=str, help=\"The path to the normailzation  input file.\")\n    parser.add_argument(\"--normailzationout\", type=str, help=\"The path to the  output file.\")\n    parser.add_argument('--log_file', '-l', type=str, help='Log file path (optional)')\n    parser.add_argument('--config', type=str, default='config.yaml', help='Configuration file path (default: config.yaml)')\n    args = parser.parse_args()\n\n    # 加载配置\n    config = load_config(args.config)\n\n    # 配置日志并获取 logger 实例\n    logger = setup_logging(args.log_file)\n\n    makeformat(args.inputfile, args.normailzationout, logger,config)\n\nif __name__ == \"__main__\":\n    main()",
    "./pfam_module.py": "import subprocess\nimport argparse\nimport logging\nimport sys\nimport yaml\nfrom log_config import setup_logging\n\n# 加载YAML配置文件的函数\ndef load_config(config_file):\n    with open(config_file, 'r') as file:\n        return yaml.safe_load(file)\n\ndef run_pfam(input,pfam_params, logger):\n    logger.info('pfam analysis started.')\n\n    # 根据 blast_type 构建命令行参数\n    command = [\n        \"python3\",\n        pfam_params['pfam_path'],  \n        input, \n        pfam_params['pfamdb_path'], \n        '-out', pfam_params['out'],\n        '-cpu', str(pfam_params['cpu'])\n    ]\n\n    try:\n        subprocess.run(command, check=True)\n        logger.info(f'pfam_scan command executed successfully.')\n    except subprocess.CalledProcessError as e:\n        logger.error(f'Error executing pfam_scan command: {e}')\n        sys.exit(1)\n        \n    logger.info(f'{pfam_params[\"pfam_path\"]} analysis completed.')\n\ndef run():\n    parser = argparse.ArgumentParser(description='Run pfam analysis.')\n    parser.add_argument('--extractprotein', type=str, default='alignment_extract_protein.fasta', help='File where the complete protein sequences are saved (default: alignment_extract_protein.fasta)', metavar='PROTEIN_FILE')\n    #parser.add_argument('--blast_type', choices=['blastp', 'blastx'], default='blastp',required=True, help='Type of BLAST program to use')\n    #parser.add_argument('--query', '-q', required=True, help='Path to the query file')\n    #parser.add_argument('--pfamoutput', '-o',default='tmp_pfam_output.csv', help='Path to the output file')\n    #parser.add_argument('--evalue', '-e', default='1e-5', help='E-value threshold')\n    #parser.add_argument('--max_target_seqs', default='5000', help='Maximum number of target sequences')\n    #parser.add_argument('--matrix', default='BLOSUM62', help='Scoring matrix name (only used for blastp)')\n    parser.add_argument('--cpu', '-t', default=8, help='Number of threads')\n    parser.add_argument('--log_file', '-l', help='Log file path (optional)')\n    parser.add_argument('--config', default='config.yaml', help='Configuration file path')\n    args = parser.parse_args()\n\n    # 加载配置\n    config = load_config(args.config)\n\n    # 使用 setup_logging 配置日志\n    logger = setup_logging(log_file_path=args.log_file)\n\n    # 临时文件\n    tmp_pfam_output = \"tmp_pfam_output.csv\"\n\n    # 将命令行参数整理成字典\n    pfam_params = {\n        'pfam_path': config[\"pfam_scan\"],  \n        'pfamdb_path': config['pfam_dirextory'],\n        'cpu': args.cpu,\n        'out': tmp_pfam_output ,\n    }\n\n    run_pfam(args.extractprotein,pfam_params, logger)\n\nif __name__ == \"__main__\":\n    run()\n",
    "./pfam_scan.py": "#!/usr/bin/env python3\n\"\"\"A script to scan one or more protein sequences against the Pfam HMMs library.\n\nCopyright 2022 Andrzej Zielezinski (a.zielezinski@gmail.com)\nhttps://github.com/aziele/pfam_scan\n\"\"\"\n\nfrom __future__ import annotations\nimport argparse\nimport collections\nimport csv\nimport json\nimport multiprocessing\nimport pathlib\nimport subprocess\nimport sys\nimport tempfile\nimport typing\nimport uuid\n\n__version__ = '1.0'\n\n\nDOMAIN_INFO = [\n    'aln_start',\n    'aln_end',\n    'env_start',\n    'env_end',\n    'hmm_acc', \n    'hmm_name',\n    'type',\n    'hmm_start',\n    'hmm_end',\n    'hmm_length',\n    'score',\n    'evalue',\n    'significance',\n    'clan',\n]\nDomain = collections.namedtuple('Domain', DOMAIN_INFO)\n\n\ndef get_parser() -> argparse.ArgumentParser:\n    \"\"\"Returns an argument parser.\"\"\"\n    parser = argparse.ArgumentParser(description=f'pfam_scan.py v.{__version__}'\n    ': search a FASTA file against a library of Pfam HMMs', add_help=False)\n    # Required arguments\n    p = parser.add_argument_group('Positional arguments (required)')\n    p.add_argument('fasta_file',\n                   help='Input FASTA file name')\n    p.add_argument('pfam_dir', \n                   help='Directory containing Pfam data files')\n    # Optional arguments\n    p = parser.add_argument_group('Optional arguments')\n    p.add_argument('-out', type=argparse.FileType('w'), default=sys.stdout,\n                   help='Output file name, otherwise send to STDOUT')\n    p.add_argument('-outfmt', choices=['csv', 'json'], default='csv',\n                   help='Output format [default: %(default)s]')\n    p.add_argument('-evalue', type=float,\n                   help='E-value threshold for a predicted domain. By default, '\n                   'this option is disabled and the tool uses the bit score '\n                   'gathering (GA) threshold recommended by Pfam')\n    p.add_argument('-cpu', type=int, \n                   default=min(multiprocessing.cpu_count(), 16),\n                   help='Number of parallel CPU workers to use for multithreads'\n                   ' (hmmscan) [default: %(default)s]')\n    # Other arguments\n    p = parser.add_argument_group('Other arguments')\n    p.add_argument('-h', '--help', action='help',\n                   default=argparse.SUPPRESS,\n                   help='Print this help message and exit')\n    p.add_argument('-v', '--version', action='version',\n                   version=f'{__version__}',\n                   help=\"Print version information and exit\")\n    # Display help if the script is run without arguments.\n    if len(sys.argv[1:]) == 0:\n        parser.print_help()\n        #parser.print_usage()\n        parser.exit()\n    return parser\n\n\ndef validate_args(parser: argparse.ArgumentParser) -> argparse.Namespace:\n    \"\"\"Validates arguments provided by the user.\n\n    Returns:\n        Arguments provided by the users.\n\n    Raises:\n        argparse.ArgumentParser.error if arguments are invalid.\n    \"\"\"\n    args = parser.parse_args()\n    # Check if the input FASTA file exists.\n    fasta_path = pathlib.Path(args.fasta_file)\n    if not fasta_path.exists():\n        parser.error(f'The input file does not exist: {fasta_path}')\n\n    # Check if the Pfam database directory exists.\n    dir_path = pathlib.Path(args.pfam_dir)\n    if not dir_path.exists():\n        parser.error(f'The Pfam directory does not exist: {dir_path}')\n\n    # Find the Pfam HMM database in the Pfam directory.\n    dir_files = list(dir_path.iterdir())\n    db_paths = [f for f in dir_files if f.suffix == '.hmm']\n    if not db_paths:\n        parser.error(f'Cannot find the HMM library file in: {dir_path}')\n    if (n := len(db_paths)) > 1:\n        parser.error(f'Found {n} HMM files in {dir_path}, expect one.')\n    db_path = db_paths[0]\n\n    # Make sure we have all binaries for the HMM database.\n    for suffix in ['.h3f', '.h3i', '.h3m', '.h3p', '.dat']:\n        path = dir_path / (db_path.name + suffix)\n        if suffix == '.dat': args.dat = path\n        if not path.exists():\n            parser.error(f'Cannot find: {path}')\n    args.fasta = fasta_path\n    args.db = db_path\n    # Create the name of the temporary file for storing the hmmscan output.\n    args.temp_file = pathlib.Path(tempfile.gettempdir(), str(uuid.uuid4().hex))\n    return args\n\n\ndef read_pfam_data(\n        filename: Union[str, pathlib.Path]\n        ) -> dict[str, collections.namedtuple]:\n    \"\"\"Reads the Pfam data file to dictionary.\n\n    Args:\n        filename: Name/Path of the Pfam data file (Pfam-A.hmm.dat).\n\n    Returns:\n        A dict mapping HMM profile name to the corresponding information.\n        For example:\n\n        {'1-cysPrx_C': Data(type='Domain', clan=None, ga_seq=21.1, ga_dom=21.1),\n         'RRM': Data(type='Domain', clan=None, ga_seq=21.0, ga_dom=21.0),\n         'SOXp': Data(type='Family', clan=None, ga_seq=22.1, ga_dom=22.1)}\n    \"\"\"\n    data = {}\n    Data = collections.namedtuple('Data', ['type', 'clan', 'ga_seq', 'ga_dom'])\n    with open(filename) as fh:\n        clan = None   # Not all domains have clan assigned.\n        for line in fh:\n            if line.startswith('#=GF ID'):\n                hmm_name = line[10:-1]\n            elif line.startswith('#=GF TP'):\n                typ = line[10:-1]\n            elif line.startswith('#=GF CL'):\n                clan = line[10:-1]\n            elif line.startswith('#=GF GA'):\n                scores = line[10:-1].strip().rstrip(';').split(';')\n                ga_seq = float(scores[0])\n                ga_dom = float(scores[1])\n            elif line.startswith('//'):\n                data[hmm_name] = Data(typ, clan, ga_seq, ga_dom)\n                clan = None\n    return data\n\n\ndef run_hmmscan(args) -> subprocess.CompletedProcess:\n    \"\"\"Runs hmmscan search using the supplied arguments.\"\"\"\n    filtering = ['--cut_ga']\n    if args.evalue:\n        #filtering = ['-E', f'{args.evalue}', '--domE', f'{args.evalue}']\n        filtering = ['--domE', f'{args.evalue}']\n    cmd = [\n        'hmmscan',\n        '--notextw',\n        '--cpu',\n        f'{args.cpu}',\n        *filtering,\n        '--domtblout',\n        f'{args.temp_file}',\n        f'{args.db}',\n        f'{args.fasta}',\n    ]\n    return subprocess.run(cmd, stdout=subprocess.DEVNULL, \n                          stderr=subprocess.PIPE, text=True)\n\ndef parse_hmmscan_output(\n        filename: Union[str, pathlib.Path],\n        pfam_data: dict[str, collections.namedtuple]\n        ) -> dict[str, list[collections.namedtuple]]:\n    \"\"\"Parses hmmscan output and returns a dictionary with the domain results.\n\n    Args:\n        filename: Name/Path of the query FASTA file.\n        pfam_data: A dict containing information on HMM profiles.\n\n    Returns:\n        A dict mapping protein ids to the corresponding list of domains.\n        For example:\n\n        {'E0SP36': [\n            Domain(\n                aln_start=65, aln_end=204, env_start=65, env_end=215,\n                hmm_acc='PF00004.32', hmm_name='AAA', type='Domain',\n                hmm_start=1, hmm_end=120, hmm_length=132, score=33.3,\n                evalue=5.9e-08, significance=1, clan='CL0023'\n            ),\n            Domain(\n                aln_start=312, aln_end=391, env_start=312, env_end=392,\n                hmm_acc='PF09079.14', hmm_name='Cdc6_C', type='Domain',\n                hmm_start=1, hmm_end=83, hmm_length=84, score=83.1,\n                evalue=1.1e-23, significance=1, clan='CL0123'\n            )\n        ], \n        'G0ECS7': [\n            Domain(\n                aln_start=164, aln_end=337, env_start=164, env_end=339,\n                hmm_acc='PF01170.21', hmm_name='UPF0020', type='Domain',\n                hmm_start=1, hmm_end=195, hmm_length=197, score=96.8,\n                evalue=1.4e-27, significance=1, clan='CL0063'\n            )\n        ]}\n    \"\"\"\n    results = {}\n    with open(filename) as fh:\n        for line in fh:\n            if line.startswith('#'):\n                continue\n            cols = line.split()\n            hmm_name = cols[0]\n            seq_id = cols[3]\n            score_dom = float(cols[13])\n            score_seq = float(cols[7])\n            # Determine which domain hits are significant. The significance \n            # value is 1 if the bit scores for a domain and a sequence are \n            # greater than or equal to the curated gathering thresholds for \n            # the matching domain, 0 otherwise. \n            significance = 0\n            if (score_dom >= pfam_data[hmm_name].ga_dom and \n                score_seq >= pfam_data[hmm_name].ga_seq):\n                # Since both conditions are true, the domain hit is significant.\n                significance = 1\n            dom = Domain(\n                int(cols[17]),             # aln_start\n                int(cols[18]),             # aln_end\n                int(cols[19]),             # env_start\n                int(cols[20]),             # env_end\n                cols[1],                   # hmm_acc\n                hmm_name,                  # hmm_name\n                pfam_data[hmm_name].type,  # type\n                int(cols[15]),             # hmm_start\n                int(cols[16]),             # hmm_end\n                int(cols[2]),              # hmm_length\n                score_dom,                 # score\n                float(cols[12]),           # evalue\n                significance,              # significance\n                pfam_data[hmm_name].clan,  # clan\n            )\n            if seq_id not in results:\n                results[seq_id] = []\n            results[seq_id].append(dom)\n    # For each protein, sort domains by their start position.\n    for seq_id in results:\n        results[seq_id].sort(key=lambda x: x.aln_start)\n    return results\n\n\ndef resolve_overlapping_domains(results):\n    \"\"\"Resolves overlapping domains belonging to the same clan.\n\n    When a protein sequence region has overlapping matches to more than one\n    domains within the same clan, only the best scoring domain match is shown.\n\n    Args:\n        results: a dict with the domain results\n    \"\"\"\n    # TODO: Consider resolving overlapping domains belonging to different clans.\n    for seq_id, domains in results.items():\n        is_overlap = True\n        while is_overlap:\n            is_overlap = False\n            indexes = set()\n            for i in range(len(domains)-1):\n                j = i + 1\n                di = domains[i]  # Domain i (previous domain)\n                dj = domains[j]  # Domain j (next domain)\n                if di.clan:\n                    # Overlapping domains within the same clan\n                    if di.clan == dj.clan and dj.aln_start <= di.aln_end:\n                        idx = j if di.evalue < dj.evalue else i\n                        indexes.add(idx)\n                        is_overlap = True\n            # Filter out overlapping domains that have weak scores.\n            domains = [d for i, d in enumerate(domains) if i not in indexes]\n        results[seq_id] = domains\n    return results\n\n\ndef main():\n    parser = get_parser()\n    args = validate_args(parser)\n    pfam_data = read_pfam_data(args.dat)\n    \n    # Run hmmscan\n    process = run_hmmscan(args)\n    if process.returncode:\n        print(f'Error running hmmscan: {process.stderr.strip()}')\n        sys.exit(1)\n    \n    # Get results\n    results = parse_hmmscan_output(args.temp_file, pfam_data)\n    results = resolve_overlapping_domains(results)\n    \n    # Write/show results in csv or json format.\n    if args.outfmt == 'csv':\n        csv_out = csv.writer(args.out)\n        csv_out.writerow(['seq_id', *DOMAIN_INFO])\n        for seq_id in results:\n            for domain in results[seq_id]:\n                if not domain.clan: domain = domain._replace(clan='No_Clan')\n                domain_list = list(domain)\n                domain_list.insert(0, seq_id)\n                csv_out.writerow(domain_list)\n    elif args.outfmt == 'json':\n        for seq_id, domains in results.items():\n            results[seq_id] = [domain._asdict() for domain in domains]\n        json.dump(results, args.out, indent=2)\n    \n    # Remove a temporary file containing hmmscan output.\n    args.temp_file.unlink()\n\n\nif __name__ == '__main__':\n    main()",
    "./position_sequence_matcher.py": "\"\"\"\n此脚本专为处理FASTA格式的蛋白质序列文件设计，旨在识别并提取包含特定氨基酸位置模式的序列。\n它允许用户指定一个或多个氨基酸的位置以及相应的氨基酸类型，然后从一个大型的FASTA文件中\n查找并提取所有符合这些条件的序列。提取的序列将被保存到指定的输出文件中。\n\n主要步骤包括：\n1. 设置参考位置索引：根据第一条序列（通常作为参考序列）和用户指定的位置与氨基酸对应关系，\n   确定在序列中的实际索引位置。\n2. 提取并追加序列：对于每个序列，如果它在指定的参考位置含有正确的氨基酸，则提取该序列\n   并将其添加到结果列表中。每个提取的序列将包括序列ID和其后续的10个非\"-\"字符（氨基酸）。\n3. 从FASTA文件中查找序列：读取FASTA文件，并基于上述逻辑查找所有符合条件的序列。\n4. 将结果写入输出文件：所有提取的序列将被写入到用户指定的输出文件中，每个序列一行，\n   序列ID前带有'>'标记。\n\n使用示例：\n    \n\n注意：在使用本脚本前，请确保输入的FASTA文件格式正确，并且已经准备好了指定的位置和氨基酸信息。\n\"\"\"\n\nimport os\nimport argparse\n\ndef parse_position_arg(position_str):\n    \"\"\"\n    解析命令行参数中的位置和氨基酸信息，支持否定条件。\n    参数 position_str 是一个字符串列表，每个元素格式为 '位置:氨基酸' 或 '位置!氨基酸'。\n    返回一个字典，键为位置（整数），值为对应的氨基酸（字符串）及是否为否定条件（布尔值）。\n    \"\"\"\n    positions = {}\n    for pos_aa in position_str:\n        if '!' in pos_aa:\n            pos, aa = pos_aa.split('!')\n            negation = True\n        else:\n            pos, aa = pos_aa.split(':')\n            negation = False\n        positions[int(pos)] = (aa, negation)\n    return positions\n\ndef set_reference_indices(sequence, positions, reference_indices):\n    \"\"\"\n    根据第一条序列（参考序列）设置参考位置索引。\n    \n    Args:\n        sequence (str): 当前处理的序列。\n        positions (dict): 指定的位置和对应的氨基酸。\n        reference_indices (dict): 存储序列中特定位置的索引和对应的氨基酸。\n    \"\"\"\n    count = 0  # 计数非\"-\"（gap）字符\n    for i, char in enumerate(sequence):\n        if char != '-':\n            count += 1\n            if count in positions:\n                reference_indices[i] = positions[count]  # 保存实际索引位置和期望的氨基酸\n\ndef extract_and_append_sequence(seq_id, sequence, reference_indices, rf_sequences):\n    \"\"\"\n    提取满足条件的序列并添加到结果列表中，包括处理否定条件。\n    \"\"\"\n    valid_sequence = True\n    for idx, (aa, negation) in reference_indices.items():\n        if negation:\n            if sequence[idx] == aa:  # 如果是否定条件且匹配，则序列不满足条件\n                valid_sequence = False\n                break\n        else:\n            if sequence[idx] != aa:  # 如果不满足正常条件，则序列不满足条件\n                valid_sequence = False\n                break\n\n    if valid_sequence:\n        first_index = min(reference_indices.keys())\n        rf_seq, count = \"\", 0\n        for i in range(first_index, len(sequence)):\n            if sequence[i] != '-':\n                rf_seq += sequence[i]\n                count += 1\n                if count == 10:\n                    break\n        if count == 10:\n            rf_sequences.append((seq_id, rf_seq))\n\ndef find_sequences_with_rf(fasta_file, positions):\n    \"\"\"\n    从FASTA文件中找到包含特定参考位置氨基酸的序列。\n    \n    Args:\n        fasta_file (str): 输入的FASTA文件路径。\n        positions (dict): 指定的位置和对应的氨基酸。\n        \n    Returns:\n        list: 包含满足条件的序列ID和序列数据的列表。\n    \"\"\"\n    with open(fasta_file, 'r') as f:\n        lines = f.readlines()\n\n    rf_sequences = []\n    reference_indices = {}\n    current_id = None\n    current_sequence = \"\"\n    first_id = None\n\n    for line in lines:\n        line = line.strip()\n        if line.startswith(\">\"):\n            if current_id and current_id != first_id:\n                extract_and_append_sequence(current_id, current_sequence, reference_indices, rf_sequences)\n            current_id = line[1:]\n            if first_id is None:\n                first_id = current_id\n            current_sequence = \"\"\n        else:\n            if not reference_indices and current_sequence == \"\":\n                set_reference_indices(line, positions, reference_indices)\n            current_sequence += line\n\n    if current_id and current_id != first_id:\n        extract_and_append_sequence(current_id, current_sequence, reference_indices, rf_sequences)\n\n    return rf_sequences\n\ndef main(fasta_file, positions, output_file='output.txt'):\n    \"\"\"\n    主函数：处理输入FASTA文件，并将结果写入输出文件。\n    \n    Args:\n        fasta_file (str): 输入的FASTA文件路径。\n        positions (dict): 指定的位置和对应的氨基酸。\n        output_file (str): 结果输出文件的路径。\n    \"\"\"\n    # 输入fasta文件路径\n    fasta_file = args.input\n    # 输入序列位置信息\n    positions = parse_position_arg(args.position)\n    # 获取所有满足条件的序列的ID以及相应的锚点序列信息\n    rf_sequences = find_sequences_with_rf(fasta_file, positions)\n    #保存到文件\n    with open(output_file, 'w') as out_file:\n        for sequence_id, anchor_seq in rf_sequences:\n            out_file.write(f\">{sequence_id} {anchor_seq}\\n\")\n    #print(\"结果已保存到output.txt文件中\")\n\n    parser = argparse.ArgumentParser(description=\"Process protein sequences\")\n    parser.add_argument(\"--input\", help=\"input fasta file\")\n    parser.add_argument(\"--position\", nargs='+', type=str, help=\"positions and corresponding amino acids\")\n    args = parser.parse_args()\n#    matches_protein = 'matches-protein.fasta'\n#    matches_cds = 'matches-cds.fasta'\n\nif __name__ == \"__main__\":\n\n    main()",
    "./process_hmmresult.py": "#对hmmseach结果进行筛选，覆盖原始结果文件\n\nimport pandas as pd\nimport re\nimport argparse\nimport shutil\nimport yaml\nfrom log_config import setup_logging\nimport sys\n\n# 加载YAML配置文件的函数\ndef load_config(config_file):\n    with open(config_file, 'r') as file:\n        return yaml.safe_load(file)\n\ndef excat_hmm_postition(input_file, output_file,hmm_evalue,hmm_score):\n    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n        # 初始化字典来存储数据\n        data = {}\n        \n        for line in infile:\n            line = line.strip()\n            # 匹配细菌名称行\n            match_bacteria = re.match(r'^>> (\\S+)\\s+.*', line)\n            if match_bacteria:\n                bacteria = match_bacteria.group(1)\n                # 初始化或重置细菌数据\n                if bacteria not in data:\n                    data[bacteria] = {'domains': 0, 'score': 0, 'evalue': 0,\n                                      'hmmfrom': 50000, 'hmmto': 0,\n                                      'alignfrom': 50000, 'alignto': 0}\n            \n            # 匹配数据行\n            match_data = re.match(r'(\\d) \\!\\s+(\\S+)\\s+(\\S+)\\s+(\\S+)\\s+(\\S+)\\s+(\\d+)\\s+(\\d+)\\s+.*\\s+(\\d+)\\s+(\\d+)\\s+.*\\s+(\\d+)\\s+(\\d+).*\\S+', line)\n            if match_data:\n                domain, score, evalue, hmmfrom, hmmto, alignfrom, alignto = match_data.groups()[0], float(match_data.groups()[1]), float(match_data.groups()[3]), int(match_data.groups()[5]), int(match_data.groups()[6]), int(match_data.groups()[7]), int(match_data.groups()[8])\n                # 更新数据\n                data[bacteria]['domains'] += 1\n                data[bacteria]['score'] += score\n                data[bacteria]['evalue'] += evalue\n                data[bacteria]['hmmfrom'] = min(data[bacteria]['hmmfrom'], hmmfrom)\n                data[bacteria]['hmmto'] = max(data[bacteria]['hmmto'], hmmto)\n                data[bacteria]['alignfrom'] = min(data[bacteria]['alignfrom'], alignfrom)\n                data[bacteria]['alignto'] = max(data[bacteria]['alignto'], alignto)\n\n        # 写入表头\n        outfile.write(\"Bac_protein\\tdomains\\tscore\\tmean_evalue\\thmm_from\\thmm_to\\talign_from\\talign_to\\n\")\n        \n        # 写入数据\n        for bacteria, info in data.items():\n            if info['evalue'] <= float(hmm_evalue) and info['score'] >= hmm_score :\n                mean_evalue = info['evalue'] / info['domains']\n                hmm_cov = (abs(info['hmmto'] - info['hmmfrom']) + 1) / 63\n                outfile.write(f\"{bacteria}\\t{info['domains']}\\t{info['score']}\\t{mean_evalue}\\t{info['hmmfrom']}\\t{info['hmmto']}\\t{info['alignfrom']}\\t{info['alignto']}\\n\")\n\n\ndef compare_files_and_keep_df1_only(file1, file2, output_file, logger):\n#与blast结果相比较，将只属于blast结果、不属于hmm的物种的GCF删除\n\n    # 读取文件1，假设第一行是标题行\n    print(file1)\n    df1 = pd.read_csv(file1, sep='\\t', header=0)\n    \n    # 读取文件2，并只处理第一列\n    df2 = pd.read_csv(file2, sep='\\t', header=0, usecols=['Bac_protein'])\n    \n    # 使用正则表达式提取每个条目的第二个_之前的内容\n    #df2['Bac_protein'] = df2['Bac_protein'].apply(lambda x: re.match(r'(.*?_.*?)_.*', x).group(1))\n    df2['Bac_protein'] = df2['Bac_protein']\n    \n    # 对df2的'Bac_protein'列去重\n    #unique_df2_values = df2['Bac_protein'].drop_duplicates()\n    unique_df2_values = df2['Bac_protein']\n\n    # 筛选df1中第二列存在于df2处理后并去重的'Bac_protein'列的行\n    filtered_df1 = df1[df1[df1.columns[1]].isin(unique_df2_values)]\n\n    # 保存筛选后的结果，包含标题行\n    filtered_df1.to_csv(output_file, index=False, sep='\\t')\n    \n    # 统计行数\n    rows_in_file1 = len(df1.index)\n    rows_in_output = len(filtered_df1.index)\n    difference = rows_in_file1 - rows_in_output\n\n    #打印日志\n    #print(f\"Merged result saved to {output_file}\")\n    logger.info(f\"Merged result saved to {output_file}\")\n    #print(f\"Rows in file1: {rows_in_file1}\")\n    logger.info(f\"blast organison in file1: {rows_in_file1}\")\n    #print(f\"Rows in output file: {rows_in_output}\")\n    logger.info(f\"HMM filtered in output file: {rows_in_output}\")\n    #print(f\"Difference in rows: {difference}\")\n    logger.info(f\"Difference in rows: {difference}\")\n\ndef main():\n    # 设置命令行参数\n    parser = argparse.ArgumentParser(description='Compare two files and keep rows from file1 that match patterns from file2, including the header of file1.')\n    parser.add_argument('--hmmsearch_output', type=str, help='Output file name for hmmsearch results.', required=True)\n    parser.add_argument('--blastoutput',  type=str, help='Path to the BLAST output file', metavar='BLAST_OUTPUT')\n    parser.add_argument('--hmm_evalue', default='1e-4',type=str, help='hmmsearch evalue  (default:1e-4)', metavar='evalue')\n    parser.add_argument('--hmm_score', default='200',type=int, help='hmmsearch score  (default:200)', metavar='score')\n    parser.add_argument('--config', type=str, default='config.yaml', help='Configuration file path (default: config.yaml)', metavar='CONFIG_FILE')\n    parser.add_argument('--log_file', '-l', type=str, help='Optional log file path', metavar='LOG_FILE')\n    parser.add_argument('--hmm_screen_output_file', help='The output file path to save the filtered results.')\n\n    # 解析命令行参数\n    args = parser.parse_args()\n\n    # 加载配置\n    config = load_config(args.config)\n\n    # 配置日志并获取 logger 实例\n    logger = setup_logging(args.log_file)\n\n    # 临时文件\n    tmp_hmmsearch_excat = 'tmp_hmmsearch_excat.csv'\n    tmp_with_hmm_normailzationout = 'tmp_with_hmm_normailzationout.csv'\n\n    # 使用配置文件中的默认值，如果命令行中提供了值，则覆盖\n    hmm_evalue = args.hmm_evalue if args.hmm_evalue is not None else config['hmm_evalue']\n    hmm_score = args.hmm_score if args.hmm_score is not None else config['hmm_score']\n\n    # 调用函数执行比较和保存结果\n    excat_hmm_postition(args.hmmsearch_output, tmp_hmmsearch_excat, hmm_evalue, hmm_score)\n    compare_files_and_keep_df1_only(args.blastoutput, tmp_hmmsearch_excat, tmp_with_hmm_normailzationout, logger)\n    # 覆盖原始的normailzationout\n    shutil.move(tmp_with_hmm_normailzationout, args.blastoutput)\n\n\nif __name__ == '__main__':\n    main()\n\n",
    "./process_pfamresult.py": "#对pfam结果进行筛选，覆盖原始结果文件\n\nimport pandas as pd\nimport re\nimport argparse\nimport shutil\nimport yaml\nfrom log_config import setup_logging\nimport sys\n\n# 加载YAML配置文件的函数\ndef load_config(config_file):\n    with open(config_file, 'r') as file:\n        return yaml.safe_load(file)\n    \ndef pfam_compare_files_and_keep_df1_only(pfamfile, blastoutfile, keys , output_file, logger):\n#\n    # 加载PFAM文件\n    pfam_df = pd.read_csv(pfamfile, sep=',')\n    # 筛选出hmm_name为Mur_ligase_C的记录，并处理seq_id列\n    filtered_pfam_df = pfam_df[pfam_df['hmm_name'].isin(keys)].copy()\n    #filtered_pfam_df['processed_seq_id'] = filtered_pfam_df['seq_id'].apply(lambda x: '_'.join(x.split('_')[:2]))\n    filtered_pfam_df['processed_seq_id'] = filtered_pfam_df['seq_id']\n    # 加载BLAST文件\n    blast_df = pd.read_csv(blastoutfile, sep='\\t')\n    # 筛选BLAST文件中存在于处理后PFAM seq_id中的行\n    #filtered_blast_df = blast_df[blast_df.iloc[:,1].isin(filtered_pfam_df['processed_seq_id'])]\n    filtered_blast_df = blast_df[blast_df.iloc[:,1].apply(lambda x: x.split('_**_')[-1]).isin(filtered_pfam_df['processed_seq_id'])]\n    # 找出被删除的行\n    #removed_blast_df = blast_df[~blast_df.iloc[:,1].isin(filtered_pfam_df['processed_seq_id'])]\n    removed_blast_df = blast_df[~blast_df.iloc[:,1].apply(lambda x: x.split('_**_')[-1]).isin(filtered_pfam_df['processed_seq_id'])]\n    # 保存筛选后的结果，包含标题行\n    filtered_blast_df.to_csv(output_file, index=False, sep='\\t')\n\n    # 统计行数\n    rows_in_filtered = len(filtered_blast_df.index)\n    rows_in_removed = len(removed_blast_df.index)\n    #difference = rows_in_file1 - rows_in_output\n\n    #打印日志\n    logger.info(f\"After screening, {rows_in_removed} sequences were removed not with {keys}\")\n    logger.info(f\"After screening, {rows_in_filtered} sequences were retained centain {keys}\")\n    #logger.info(f\"Rows in output file: {rows_in_output}\")\n    #logger.info(f\"Difference in rows: {difference}\")\n\ndef main():\n    # 设置命令行参数\n    parser = argparse.ArgumentParser(description='Compare two files and keep rows from file1 that match patterns from file2, including the header of file1.')\n    parser.add_argument('--pfamfile', type=str, required=True, help='Output file name for hmmsearch results.')\n    parser.add_argument('--blastoutfile', type=str, required=True, help='blast Normalization output file name', metavar='NORM_OUTPUT')\n    parser.add_argument('--pfamkey', nargs='+', type=str, help='One or more pfam domain keys , Example: \"Mur_ligase_C Another_HMM\" ', required=True)\n    parser.add_argument('--log_file', '-l', type=str, help='Optional log file path', metavar='LOG_FILE')\n    parser.add_argument('--output_file', help='The output file path to save the filtered results.')\n\n    # 解析命令行参数\n    args = parser.parse_args()\n\n    # 加载配置\n    logger = setup_logging(args.log_file)\n    #config = load_config(args.config)  \n\n    # 临时文件\n    tmp_pfam_excat = 'tmp_pfam_excat.csv'\n\n\n    # 调用函数执行比较和保存结果\n    #excat_hmm_postition(args.target_sequence_file, tmp_hmmsearch_excat)\n    pfam_compare_files_and_keep_df1_only(args.pfamfile, args.blastoutfile, args.pfamkey, tmp_pfam_excat,logger)\n    # 覆盖原始的normailzationout\n    shutil.move(tmp_pfam_excat, args.blastoutfile)\n\n    #删除临时文件\n\n\nif __name__ == '__main__':\n    main()\n\n",
    "./protein_operation.py": "import argparse\n\ndef filter_duplicate_proteins(input_file, output_file):\n    \"\"\"\n    从输入文件中筛选出唯一的蛋白质，并写入输出文件。\n\n    Args:\n        input_file (str): 输入文件路径。\n        output_file (str): 输出文件路径。\n    \"\"\"\n    unique_proteins = {}\n    with open(input_file, 'r') as f:\n        lines = f.readlines()\n        for line in lines:\n            if line.startswith(\">\"):\n                protein_id = line.split()[0]\n                wp_id = protein_id.split(\"_**_\")[-1].split(\"\\t\")[0]  # 获取蛋白质ID中的特定部分\n                if wp_id not in unique_proteins:\n                    unique_proteins[wp_id] = True\n\n    with open(output_file, 'w') as f:\n        l = 0\n        for line in lines:\n            if line.startswith(\">\"):\n                protein_id = line.split()[0]\n                wp_id = protein_id.split(\"_**_\")[-1].split(\"\\t\")[0]  # 获取蛋白质ID中的特定部分\n                if wp_id in unique_proteins:\n                    l = 1\n                    f.write(\">\" + wp_id + \"\\n\")  # 只保留 WP 部分\n                    del unique_proteins[wp_id]\n            elif l == 1 :\n                f.write(line)\n                l = 0\n\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Tool for filtering duplicate proteins, merging FASTA files, and running Clustal Omega for multiple sequence alignment.\")\n    parser.add_argument('--input', '-i', type=str, help='input file path (optional)')\n    parser.add_argument('--output', '-o', type=str, help='output file path (optional)')\n    #parser.add_argument('--log_file', '-l', type=str, help='Log file path (optional)')\n    #parser.add_argument('--config', type=str, default='config.yaml', help='Configuration file path (default: config.yaml)')\n\n    args = parser.parse_args()\n\n    filter_duplicate_proteins(args.input, args.output)  # 过滤重复的蛋白质ID并写入临时文件\n\nif __name__ == \"__main__\":\n    main()\n\n",
    "./README.md": "conda install pyyaml\nconda install pandas",
    "./run_clustalo_process.py": "import argparse\nimport os\nimport sys\nimport yaml \nimport subprocess\nfrom FileOperation import remove_file\nfrom log_config import setup_logging\n\n# 加载YAML配置文件的函数\ndef load_config(config_file):\n    with open(config_file, 'r') as file:\n        return yaml.safe_load(file)\n\ndef filter_duplicate_proteins(input_file, output_file):\n    \"\"\"\n    从输入文件中筛选出唯一的蛋白质，并写入输出文件。\n\n    Args:\n        input_file (str): 输入文件路径。\n        output_file (str): 输出文件路径。\n    \"\"\"\n    unique_proteins = {}\n    with open(input_file, 'r') as f:\n        lines = f.readlines()\n        for line in lines:\n            if line.startswith(\">\"):\n                protein_id = line.split()[0]\n                wp_id = protein_id.split(\"_**_\")[-1].split(\"\\t\")[0]  # 获取蛋白质ID中的特定部分\n                if wp_id not in unique_proteins:\n                    unique_proteins[wp_id] = True\n\n    with open(output_file, 'w') as f:\n        l = 0\n        for line in lines:\n            if line.startswith(\">\"):\n                protein_id = line.split()[0]\n                wp_id = protein_id.split(\"_**_\")[-1].split(\"\\t\")[0]  # 获取蛋白质ID中的特定部分\n                if wp_id in unique_proteins:\n                    l = 1\n                    f.write(\">\" + wp_id + \"\\n\")  # 只保留 WP 部分\n                    del unique_proteins[wp_id]\n            elif l == 1 :\n                f.write(line)\n                l = 0\n\ndef merge_fasta_files(file1, file2, output_file):\n    \"\"\"\n    将两个 FASTA 格式的文件合并成一个输出文件。\n\n    Args:\n        file1 (str): 第一个输入文件路径。\n        file2 (str): 第二个输入文件路径。\n        output_file (str): 输出文件路径。\n    \"\"\"\n    with open(output_file, 'w') as output:\n        with open(file1, 'r') as f1:\n            output.write(f1.read())\n        with open(file2, 'r') as f2:\n            output.write(f2.read())\n\ndef run_clustalo(input_file, output_file, num_threads, logger, config):\n    \"\"\"\n    运行 Clustal Omega 进行多序列比对。\n\n    Args:\n        input_file (str): 输入文件路径。\n        output_file (str): 输出文件路径。\n        num_threads (int): 使用的线程数，默认为 8。\n    \"\"\"\n    logger.info(f\"Starting clustalo for query file: {input_file} with {num_threads} threads\")\n    command = [\n        config['clustalo_path'],\n        '--infile', input_file,\n        '-o', output_file,\n        '--threads', str(num_threads),\n        '--force'\n    ]    \n    \n    try:\n        subprocess.run(command, check=True)\n        logger.info(\"clustalo command executed successfully.\")\n    except subprocess.CalledProcessError as e:\n        logger.error(f\"Error executing clustalo command: {e}\")\n        sys.exit(1)  # 出错时退出程序\n\n#def remove_file(file):\n#    \"\"\"\n#    删除指定文件。\n#\n#    Args:\n#        file (str): 要删除的文件路径。\n#    \"\"\"\n#    if os.path.exists(file):\n#        os.remove(file)\n\n\ndef find_start_end_positions(alignment, sequence_id):\n    \"\"\"\n    查找序列在多序列比对结果中的起始和结束位置。\n\n    Args:\n        alignment (dict): 多序列比对结果，格式为序列ID到序列的映射字典。\n        sequence_id (str): 要查找的序列ID。\n\n    Returns:\n        tuple: 起始位置和结束位置的元组。\n    \"\"\"\n    start_position = None\n    end_position = None\n\n    # 找到序列的起始位置\n    for i in range(len(alignment[sequence_id])):\n        if alignment[sequence_id][i] != '-':\n            start_position = i\n            break\n\n    # 找到序列的结束位置\n    for i in range(len(alignment[sequence_id]) - 1, -1, -1):\n        if alignment[sequence_id][i] != '-':\n            end_position = i\n            break\n\n    return start_position, end_position\n\ndef find_end_position(alignment, sequence_id):\n    \"\"\"\n    查找序列在多序列比对结果中的结束位置。\n\n    Args:\n        alignment (dict): 多序列比对结果，格式为序列ID到序列的映射字典。\n        sequence_id (str): 要查找的序列ID。\n\n    Returns:\n        int: 结束位置。\n    \"\"\"\n    sequence = alignment[sequence_id]\n    end_position = None\n    for i in range(len(sequence) - 1, -1, -1):\n        if sequence[i] != '-':\n            end_position = i + 3  # 找到最后一个非'-'的位置后，往后数三位作为结束位置\n            break\n    return end_position\n\ndef trim_alignment(alignment, start_position, end_position, target_sequence_id, padding=3):\n    \"\"\"\n    裁剪多序列比对结果，只保留指定范围内的序列。\n\n    Args:\n        alignment (dict): 多序列比对结果，格式为序列ID到序列的映射字典。\n        start_position (int): 起始位置。\n        end_position (int): 结束位置。\n        target_sequence_id (str): 要裁剪的序列ID。\n        padding (int): 裁剪时的填充大小，默认为 3。\n\n    Returns:\n        dict: 裁剪后的多序列比对结果。\n    \"\"\"\n    trimmed_alignment = {}\n    for seq_id, seq in alignment.items():\n        trimmed_seq = seq[start_position - padding:end_position + padding + 1]  # 加一是因为切片时右边界不包含\n        trimmed_alignment[seq_id] = trimmed_seq\n    # 将指定序列移动到字典的第一个位置\n    target_sequence = trimmed_alignment.pop(target_sequence_id)\n    trimmed_alignment = {target_sequence_id: target_sequence, **trimmed_alignment}\n    return trimmed_alignment\n\ndef read_alignment_from_file(file):\n    \"\"\"\n    从文件中读取多序列比对结果。\n\n    Args:\n        file (str): 输入文件路径。\n\n    Returns:\n        dict: 多序列比对结果，格式为序列ID到序列的映射字典。\n    \"\"\"\n    alignment = {}\n    with open(file, 'r') as f:\n        current_seq_id = None\n        current_seq = ''\n        for line in f:\n            if line.startswith('>'):\n                if current_seq_id:\n                    alignment[current_seq_id] = current_seq\n                current_seq_id = line.strip()\n                current_seq = ''\n            else:\n                current_seq += line.strip()\n        if current_seq_id:\n            alignment[current_seq_id] = current_seq\n    return alignment\n\ndef write_alignment_to_file(alignment, output_file):\n    \"\"\"\n    将多序列比对结果写入文件。\n\n    Args:\n        alignment (dict): 多序列比对结果，格式为序列ID到序列的映射字典。\n        output_file (str): 输出文件路径。\n    \"\"\"\n    with open(output_file, 'w') as f:\n        for seq_id, seq in alignment.items():\n            f.write(seq_id + '\\n')\n            f.write(seq + '\\n')\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Tool for filtering duplicate proteins, merging FASTA files, and running Clustal Omega for multiple sequence alignment.\")\n    parser.add_argument(\"--target_sequence_file\", help=\"File containing the target sequence(150nt).\")\n    parser.add_argument('--extractprotein', type=str, default='alignment_extract_protein.fasta', help='File where the complete protein sequences are saved (default: alignment_extract_protein.fasta)', metavar='PROTEIN_FILE')\n    parser.add_argument(\"--clustalo_out\", type=str ,help=\"clustalo output file .\")\n    parser.add_argument('--log_file', '-l', type=str, help='Log file path (optional)')\n    parser.add_argument('--config', type=str, default='config.yaml', help='Configuration file path (default: config.yaml)')\n\n    args = parser.parse_args(args)\n\n    output_protein_file = args.extractprotein\n    target_sequence_file = args.target_sequence_file\n    temp_filtered_file = \"temp_filtered.fasta\"\n    merged_file = \"merged.fasta\"\n    clustal_output_file = \"clu-out.fasta\"\n    num_threads = 8\n\n    # 加载配置\n    config = load_config(args.config)\n\n    # 使用 setup_logging 配置日志\n    logger = setup_logging(log_file_path=args.log_file)\n\n    # 读取目标序列文件，获取序列ID\n    with open(target_sequence_file, 'r') as f:\n        target_sequence_id = f.readline().strip()\n\n    remove_file(merged_file)  # 删除之前的 merged.fasta 文件\n    filter_duplicate_proteins(output_protein_file, temp_filtered_file)  # 过滤重复的蛋白质ID并写入临时文件\n    merge_fasta_files(temp_filtered_file, target_sequence_file, merged_file)\n    run_clustalo(merged_file, clustal_output_file, num_threads, logger, config)\n    remove_file(merged_file)  # 删除临时合并的文件\n    remove_file(temp_filtered_file)  # 删除临时过滤的文件\n\n    if args.clustalo_out : \n        clustal_output_file = args.clustalo_out\n\n    # 从比对结果中裁剪指定序列并写入文件\n    alignment = read_alignment_from_file(clustal_output_file)\n    start_position, end_position = find_start_end_positions(alignment, target_sequence_id)\n    end_position = find_end_position(alignment, target_sequence_id)\n    trimmed_alignment = trim_alignment(alignment, start_position, end_position, target_sequence_id)\n    write_alignment_to_file(trimmed_alignment, clustal_output_file)\n\nif __name__ == \"__main__\":\n    main()\n\n",
    "./sequence_pattern_extractor.py": "\"\"\"\n此脚本用于从FASTA格式的蛋白质序列文件中，基于特定的名称列表，查找并提取匹配的序列记录。它首先从一个文本文件中读取序列名称，\n然后在一个较大的FASTA文件中查找这些名称对应的序列，并将所有找到的匹配序列保存到另一个文件中。这个过程支持生物信息学分析，\n尤其是在需要从大量序列数据中筛选特定序列时。\n\n使用步骤如下：\n1. 将需要查找的序列名称列表保存在一个文本文件中，每个名称位于以'>'开头的行。\n2. 准备一个FASTA格式的文件，其中包含了大量的蛋白质序列。\n3. 指定一个输出文件路径，用于保存匹配的序列。\n\n脚本包含四个主要函数：\n- read_names_from_file: 从文件中读取名称列表。\n- find_matching_sequences: 在FASTA文件中查找与名称列表匹配的序列。\n- save_matches_to_file: 将找到的匹配序列保存到指定的文件中。\n- main: 将上述步骤组织起来，从读取名称到保存匹配序列的完整流程。\n\n此脚本的设计目的是为了提高生物信息学数据处理的效率，特别是在处理大规模序列数据时，通过自动化筛选特定序列，来支持研究和分析。\n\n使用示例：\n    \n\n注意：在使用本脚本前，请确保你已经有了包含名称的文本文件和FASTA格式的蛋白质序列文件。\n\"\"\"\n\nimport argparse\nfrom log_config import setup_logging\n\ndef read_names_from_file(file_path):\n    \"\"\"\n    从文件中读取并返回所有以'>'开头的行的第一个单词，去掉'>'。\n\n    Args:\n        file_path (str): 文件的路径。\n\n    Returns:\n        list: 包含所有提取的名称的列表。\n    \"\"\"\n    names = []\n    with open(file_path, 'r') as file:\n        for line in file:\n            if line.startswith('>'):\n                name = line.split()[0][1:]\n                names.append(name)\n    return names\n\ndef find_matching_sequences(input_fasta_file, names):\n    \"\"\"\n    在FASTA文件中查找名称匹配的序列。\n\n    Args:\n        input_fasta_file (str): FASTA文件的路径。\n        names (list): 需要匹配的名称列表。\n\n    Returns:\n        list: 包含匹配的序列记录的列表，每个记录是一个元组，包含头部和序列列表。\n    \"\"\"\n    matches = []\n    record = None\n    with open(input_fasta_file, 'r') as fasta_file:\n        for line in fasta_file:\n            if line.startswith('>'):\n                if record:\n                    for name in names:\n                        if name in record[0]:\n                            matches.append(record)\n                            break\n                record = (line, [])\n            elif record:\n                record[1].append(line)\n\n        # 检查最后一个记录是否匹配\n        if record:\n            for name in names:\n                if name in record[0]:\n                    matches.append(record)\n                    break\n    return matches\n\ndef save_matches_to_file(matches, output_file_path):\n    \"\"\"\n    将所有匹配的序列保存到文件中。\n\n    Args:\n        matches (list): 匹配的序列记录列表。\n        output_file_path (str): 输出文件的路径。\n    \"\"\"\n    with open(output_file_path, 'w') as match_file:\n        for match in matches:\n            match_file.write(match[0])\n            match_file.writelines(match[1])\n#    logger.info(f'找到的符合条件的蛋白质序列保存在：{output_file_path}')\n\ndef main():\n\n    parser = argparse.ArgumentParser(description=\"Process protein sequences\")\n    parser.add_argument(\"--input_file\", help=\"input fasta file\")\n    parser.add_argument(\"--protein_db\", help=\"input protein fasta file\")\n    parser.add_argument(\"--output_file\", help=\"output protein fasta file\")\n    args = parser.parse_args()\n    \n    # 使用 setup_logging 配置日志\n    logger = setup_logging(log_file_path=args.log_file)\n\n    # 定义文件路径\n    input_names_file = args.input_file\n    input_fasta_file = args.protein_db\n    output_file_path = args.output_file\n\n    names = read_names_from_file(input_names_file)\n    matches = find_matching_sequences(input_fasta_file, names)\n    save_matches_to_file(matches, output_file_path,logger)\n\nif __name__ == \"__main__\":\n\n    main()\n",
    "./species_checker_format.py": "import pandas as pd\nimport argparse\nimport yaml\nfrom log_config import setup_logging\n\ndef load_config(config_file):\n    with open(config_file, 'r') as file:\n        return yaml.safe_load(file)\n\nclass SpeciesChecker:\n    def __init__(self, fileA_path, fileB_path, gcf_annotation_path, output_path, log):\n        self.fileA_path = fileA_path\n        self.fileB_path = fileB_path\n        self.gcf_annotation_path = gcf_annotation_path\n        self.output_path = output_path\n        self.log = log\n\n    def extract_species_and_proteins(self, file_path):\n        species_proteins = {}\n        try:\n            with open(file_path, 'r') as file:\n                for line in file:\n                    if line.startswith('>'):\n                        parts = line.split('_**_')\n                        gcf_id = '_'.join(parts[0].split('>')[1].split('_')[:2])\n                        protein_id = parts[1].split()[0]\n                        species_proteins.setdefault(gcf_id, set()).add(protein_id)\n            self.log.info(f\"Successfully extracted species and proteins from {file_path}\")\n        except Exception as e:\n            self.log.error(f\"Error extracting from {file_path}: {e}\")\n        return species_proteins\n\n    def extract_gcf_annotations(self):\n        try:\n            df = pd.read_csv(self.gcf_annotation_path)\n            gcf_annotations = df.set_index('GCF Prefix').to_dict('index')\n            self.log.info(\"Successfully extracted GCF annotations\")\n            return gcf_annotations\n        except Exception as e:\n            self.log.error(f\"Failed to extract GCF annotations: {e}\")\n            return {}\n\n    def check_presence(self, species_proteins, file_path):\n        presence_info = {}\n        try:\n            with open(file_path, 'r') as file:\n                file_content = file.read()\n                for gcf_id, proteins in species_proteins.items():\n                    matching_proteins = [protein for protein in proteins if protein in file_content]\n                    presence_info[gcf_id] = 'yes:' + ','.join(matching_proteins) if matching_proteins else 'no'\n            self.log.info(f\"Checked presence for {file_path}\")\n        except Exception as e:\n            self.log.error(f\"Error checking presence in {file_path}: {e}\")\n        return presence_info\n\n    def generate_report(self):\n        try:\n            species_proteins_A = self.extract_species_and_proteins(self.fileA_path)\n            species_proteins_B = self.extract_species_and_proteins(self.fileB_path)\n            gcf_annotations = self.extract_gcf_annotations()\n\n            presence_A = self.check_presence(species_proteins_A, self.fileA_path)\n            presence_B = self.check_presence(species_proteins_B, self.fileB_path)\n\n            all_gcf_ids = set(presence_A.keys()) | set(presence_B.keys())\n\n            columns_order = ['GCF_ID', 'strain_Name', 'taxon','Species', 'Genus', 'Family', 'Order', 'Class', 'Phylum', 'homology', 'homology_with_site']\n\n            data = []\n            for gcf_id in all_gcf_ids:\n                annotation = gcf_annotations.get(gcf_id, {key: 'N/A' for key in columns_order[1:-2]})\n                row = {\n                    'GCF_ID': gcf_id,\n                    'homology': presence_A.get(gcf_id, 'no'),\n                    'homology_with_site': presence_B.get(gcf_id, 'no'),\n                    **annotation\n                }\n                ordered_row = {col: row.get(col, 'N/A') for col in columns_order}\n                data.append(ordered_row)\n\n            # 创建DataFrame\n            df = pd.DataFrame(data)\n            df = df[columns_order]\n            df.to_csv(self.output_path, index=False, sep='\\t')\n            #self.log.info(f\"Output saved to {self.output_path}\")\n\n            # 计算含有\"yes\"的行数\n            fileA_yes_count = df['homology'].apply(lambda x: 'yes' in x).sum()\n            fileB_yes_count = df['homology_with_site'].apply(lambda x: 'yes' in x).sum()\n\n            # 计算差值\n            difference = abs(fileA_yes_count - fileB_yes_count)\n\n            # 日志输出结果\n            self.log.info(f\"homology fileA has {fileA_yes_count} rows with 'yes'.\")\n            self.log.info(f\"homology_with_site fileB has {fileB_yes_count} rows with 'yes'.\")\n            self.log.info(f\"The difference in 'yes' rows between FileA and FileB is {difference}.\")\n\n\n            df = pd.DataFrame(data)\n            df = df[columns_order]\n            df.to_csv(self.output_path, index=False, sep='\\t')\n            self.log.info(f\"Output saved to {self.output_path}\")\n        except Exception as e:\n            self.log.error(f\"Failed to generate report: {e}\")\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Check species presence in files, record associated proteins, and include detailed GCF annotations.\")\n    parser.add_argument('--extractprotein', required=True, type=str, help='Output file for complete protein sequences.', metavar='PROTEIN_FILE')\n    parser.add_argument(\"--site_protein_output\", required=True,  type=str, help=\"Output file for matched protein sequences.\", metavar=\"PROTEIN_OUT\")\n    parser.add_argument('--config', type=str, required=True, help='Path to configuration file')\n    parser.add_argument('--log_file', '-l', help='Log file path (optional)')\n    parser.add_argument('--formatoutput', type=str, required=True, help='Path to output file')\n    args = parser.parse_args()\n\n    config = load_config(args.config)\n    logger = setup_logging(args.log_file) if args.log_file else setup_logging()\n\n    format_path = config['medusa-annotation']\n\n    checker = SpeciesChecker(args.extractprotein, args.site_protein_output, format_path, args.formatoutput, logger)\n    checker.generate_report()\n\nif __name__ == \"__main__\":\n    main()\n",
    "./test.py": ""
}